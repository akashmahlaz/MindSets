https://getstream.io/video/docs/react-native/setup/installation/expo/   , Expo
Our SDK is not available on Expo Go due to native code being required, but you can use the expo-dev-client library to run your Expo app with a development build.

Development Build
If you haven’t already, prepare your project for expo development builds.

SDK Installation
Add the Stream Video React Native SDK and its required dependencies to your project:

Terminal
npx expo install @stream-io/video-react-native-sdk \
  @stream-io/react-native-webrtc \
  @config-plugins/react-native-webrtc \
  react-native-svg \
  @react-native-community/netinfo \
  expo-build-properties
So what did we install precisely?

@stream-io/video-react-native-sdk (SVRN) is Stream’s Video SDK which contains UI components, hooks and util functions that will enable audio/video calls.
@stream-io/react-native-webrtc is a WebRTC module for React Native, SVRN depends on this dependency, it’s components and utilities to render audio/video tracks and interact with the phone’s media devices.
@config-plugins/react-native-webrtc config plugin to auto-configure @stream-io/react-native-webrtc when the native code is generated (npx expo prebuild).
react-native-svg provides SVG support to React Native, SVRN’s components and it’s icons are reliant on this dependency.
@react-native-community/netinfo is used to detect the device’s connectivity state, type and quality.
expo-build-properties is a config plugin for configuring the native build properties.
Starting from version 125.3.0 version of @stream-io/react-native-webrtc, only Expo version 50 and above is supported. If you are on an older Expo version than 50, please use 125.2.1 version of @stream-io/react-native-webrtc.

Android Specific installation
Update the minSdk version
In your app.json file add the following to the expo-build-properties plugin:

app.json
{
  "expo": {
    ...
    "plugins": [
      "expo-build-properties",
      {
        "android": {
          "minSdkVersion": 24
        }
      }
    ]
  }
}
Add config plugin
Add the config plugin for @stream-io/video-react-native-sdk and react-native-webrtc to your app.json file:

app.json
{
  "expo": {
    ...
    "plugins": [
      "@stream-io/video-react-native-sdk",
      [
        "@config-plugins/react-native-webrtc",
        {
          // add your explanations for camera and microphone
          "cameraPermission": "$(PRODUCT_NAME) requires camera access in order to capture and transmit video",
          "microphonePermission": "$(PRODUCT_NAME) requires microphone access in order to capture and transmit audio"
        }
      ]
    ]
  }
}
If Expo EAS build is not used, please do npx expo prebuild --clean to generate the native directories again after adding the config plugins.

Permissions need to be granted by the user as well. Requests for Camera and Microphone usage are automatically asked when the stream is first requested by the app. But other permissions like BLUETOOTH_CONNECT in Android need to be requested manually. However, we recommend that all necessary permissions be manually asked at an appropriate place in your app for the best user experience.

We recommend the usage of react-native-permissions library to request permissions in the app.

Run on device
iOS
In iOS simulators, recording audio or video is not supported. So always test your app on an actual device for the best experience.

Android
In Android emulators, a static video stream can be sent and so it can be used for testing. However, we recommend that you always test your app on an actual device for the best experience.

New Architecture (Fabric)
The SDK’s native modules and views are compatible with the New Architecture and Bridgeless mode through the New Renderer Interop Layers. These layers are automatically enabled when you turn on the New Architecture in React Native 0.74 and above. We recommend that you use React Native 0.74+ if you are using the New Architecture with the SDK.

Troubleshooting
GestureDetector must be used as a descendant of GestureHandlerRootView
Our SDK uses react-native-gesture-handler library if it has been installed for smooth gestures. To fix this error, wrap your entry point with or gestureHandlerRootHOC.

For example:


import { GestureHandlerRootView } from "react-native-gesture-handler";
export default function App() {
  return <GestureHandlerRootView>{/* content */}</GestureHandlerRootView>;
}
Error: Super expression must either be null or a function
This was a known issue in @stream-io/react-native-webrtc library.

It has been fixed on version 118.1.0 of the @stream-io/react-native-webrtc library.

Android Only Error: Could not find any matches for app.notifee:core:+ as no versions of app.notifee:core are available
This occurs on Expo 49+ with a monorepo configuration. Notifee is unable to find the compiled AAR android library. You can do the following workaround in your app.json to mitigate this:


const config = {
  expo: {
    // ...
    plugins: [
      [
        "expo-build-properties",
        {
          android: {
            extraMavenRepos: [
              "$rootDir/../../../node_modules/@notifee/react-native/android/libs",
            ],
          },
        },
      ],
    ],
  },
};
This will add the Notifee library to the list of repositories that Gradle will search for dependencies. Please note that the exact path for extraMavenRepos will vary depending on your project’s structure.

Reference: https://github.com/invertase/notifee/issues/808

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
React Native
Next
Quickstart
© Getstream.io, Inc. All Rights Reserved.Video and Audio
/
Docs
/
React Native
/
Client & Authentication
Client & Authentication
Client & Auth
Before joining a call, it is necessary to set up the video client. Here’s a basic example:


import { StreamVideoClient, User } from "@stream-io/video-react-native-sdk";
const user: User = { id: "sara" };
const apiKey = "my-stream-api-key";
const token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...";
const client = StreamVideoClient.getOrCreateInstance({ apiKey, token, user });
The API Key can be found in your dashboard.
The user can be either authenticated, anonymous or guest.
Note: You can store custom data on the user object, if required.
Typically, you’ll want to initialize the client when your application loads and use a context provider to make it available to the rest of your application.

Generating a token
Tokens need to be generated server side. You can use our server side SDKs to quickly add support for this. Typically, you integrate this into the part of your codebase where you log in or register users. The tokens provide a way to authenticate a user or give access to a specific set of calls.

Here are credentials to try out the app with:

Property	Value
API Key	mmhfdzb5evj2
Token	eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJodHRwczovL3Byb250by5nZXRzdHJlYW0uaW8iLCJzdWIiOiJ1c2VyL0V2ZXJsYXN0aW5nX0FyYWdvbiIsInVzZXJfaWQiOiJFdmVybGFzdGluZ19BcmFnb24iLCJ2YWxpZGl0eV9pbl9zZWNvbmRzIjo2MDQ4MDAsImlhdCI6MTc2NDQ1MjcyMCwiZXhwIjoxNzY1MDU3NTIwfQ.XS5WZORLIQFVQHrToZfwrFiV4lQnpv0AEXLDn905v9ACopy
User ID	Everlasting_Aragon
Call ID	aqhNtZMXS4Een40EbJB6l
For development purposes, you can use our Token Generator.

Different types of users
Authenticated users are users that have an account on your app.
Guest users are temporary user accounts. You can use it to temporarily give someone a name and image when joining a call.
Anonymous users are users that are not authenticated. It’s common to use this for watching a livestream or similar where you aren’t authenticated.
Guest users
This example shows the client setup for a guest user:


import { StreamVideoClient, User } from "@stream-io/video-react-native-sdk";
const user: User = { id: "jack-guest", type: "guest" };
const apiKey = "my-stream-api-key";
const client = StreamVideoClient.getOrCreateInstance({ apiKey, user });
Anonymous users
And here’s an example for an anonymous user


import { StreamVideoClient, User } from "@stream-io/video-react-native-sdk";
const user: User = { type: "anonymous" };
const apiKey = "my-stream-api-key";
const client = StreamVideoClient.getOrCreateInstance({ apiKey, user });
Anonymous users don’t establish an active web socket connection, therefore they won’t receive any events. They are just able to watch a livestream or join a call.

The token for an anonymous user should contain the call_cids field, which is an array of the call cid’s that the user is allowed to join.

Here’s an example JWT token payload for an anonymous user:


{
  "iss": "@stream-io/dashboard",
  "iat": 1726406693,
  "exp": 1726493093,
  "user_id": "!anon",
  "role": "viewer",
  "call_cids": ["livestream:123"]
}
Connecting a user and error handling
Users can be connected in two ways:

Automatically when the client is created:

const client = new StreamVideoClient({
  apiKey,
  user,
  token,
  options: {
    maxConnectUserRetries: 3,
    onConnectUserError: (err: Error, allErrors: Error[]) => {
      console.error("Failed to connect user", err, allErrors);
      // handle the connect error, i.e. ask the user to retry
      // later when they have better connection or show an error message
    },
  },
});
Manually by calling the client.connectUser() method:

const client = new StreamVideoClient({ apiKey });
try {
  await client.connectUser(user, token);
} catch (err) {
  console.error("Failed to connect user", err);
  // handle the connect error
}
Disconnecting a user
To discard a client instance, or to disconnect a user, call the client.disconnectUser() method:


await client.disconnectUser();
Client options
token or tokenProvider
To authenticate users you can either provide a string token or a tokenProvider function that returns Promise<string>.

When using a tokenProvider, the SDK will automatically execute it to refresh the token whenever the token expires.


import { StreamVideoClient, User } from "@stream-io/video-react-native-sdk";
const tokenProvider = async () => {
  const response = await fetch("/api/token");
  const data = await response.json();
  return data.token;
};
const user: User = { id: "sara" };
const apiKey = "my-stream-api-key";
const client = StreamVideoClient.getOrCreateInstance({
  apiKey,
  tokenProvider,
  user,
});
Reject incoming call when busy
You can optionally configure the client to automatically reject a incoming ringing call when the user is busy in another call. This is done by setting the option rejectCallWhenBusy to true.


import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
const client = StreamVideoClient.getOrCreateInstance({
  apiKey,
  tokenProvider,
  user,
  options: { rejectCallWhenBusy: true },
});
The SDK will play a busy tone in the caller side when the callee had rejected due to being busy.

Logging
The SDK uses pre-scoped logging mechanism for you to use. Each scope represents a specific module (or a group of modules) that produce logs.

You can set up a sink and a log level for each of these scopes individually. The mechanism will use default scope if logger cannot find settings for the accessed scope. A “sink” is a function which gets funneled logs an SDK code produces. A sink gets a log level, message and other values we pass down to it as arguments from the SDK.

Log levels are pre-defined and are ordered by severity as such:

trace: 0
debug: 1
info: 2
warn: 3
error: 4
For example; if you set a log level to a value warn for a scope event-dispatcher, you will only see errors and warnings generated by the SDK for that specific scope.

Note that our SDK only logs errors that do not get re-thrown. Using our API (producing HTTP request) might result in errors which should be handled within individual integrations.


import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
const client = StreamVideoClient.getOrCreateInstance({
  apiKey,
  token,
  user,
  options: {
    logOptions: {
      default: {
        level: "info",
      },
      coordinator: {
        level: "warn",
        sink: (logLevel, message, ...rest) => {
          switch (logLevel) {
            case "warn": {
              console.warn(message, ...rest);
              break;
            }
            case "error": {
              SuperLogger.error(message, ...rest);
            }
          }
        },
      },
    },
  },
});
You can augment the settings of the default scope if you don’t want to apply custom sinks/levels to all the SDK-defined scopes individually.


import { videoLoggerSystem } from "@stream-io/video-client";
import SuperLogger from "./SuperLogger";
videoLoggerSystem.configureLoggers({
  default: {
    level: "info",
    sink: (logLevel, message, ...rest) => {
      SuperLogger[logLevel](message, ...rest);
    },
  },
});
If you need to reset specific scopes during your application runtime to use defaults instead you can do so as follows:


import { videoLoggerSystem } from "@stream-io/video-client";
videoLoggerSystem.configureLoggers({
  "event-dispatcher": {
    level: null,
    sink: null,
  },
});
This action will only reset the event-dispatcher scope and api-client scope will stay intact (considering previous examples). If you need to reset the defaults completely, you can do so as follows:


import { videoLoggerSystem } from "@stream-io/video-client";
videoLoggerSystem.restoreDefaults();
StreamVideo context provider
You can use the StreamVideo context provider to make the SDK client available to the rest of the application. We also use the tokenProvider to show how to perform auth server-side.


import { useEffect, useState } from "react";
import {
  StreamVideo,
  StreamVideoClient,
  User,
} from "@stream-io/video-react-native-sdk";
const apiKey = "my-stream-api-key";
const user: User = { id: "sara", name: "Sara" };
export const MyApp = () => {
  const [client, setClient] = useState<StreamVideoClient>();
  useEffect(() => {
    const tokenProvider = () => Promise.resolve("<token>");
    const myClient = StreamVideoClient.getOrCreateInstance({
      apiKey,
      user,
      tokenProvider,
    });
    setClient(myClient);
    return () => {
      myClient.disconnectUser();
      setClient(undefined);
    };
  }, []);
  if (!client) return null;
  return (
    <StreamVideo client={client}>
      <MyVideoApp />
    </StreamVideo>
  );
};
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Quickstart
Next
Joining & Creating Calls
© Getstream.io, Inc. All Rights Reserved.
Contact,   Video and Audio
/
Docs
/
React Native
/
Joining & Creating Calls
Joining & Creating Calls
This guide shows how to create, join, leave, and end calls and ring calls.

Call
Call represents the main building block of our SDK. This object abstracts away the user actions, join flows and exposes the call state.

Create call
You can create a call by specifying its callType and callId:

The Call Type controls which features are enabled, and sets up permissions. You can reuse the same call multiple times. As an example, if you’re building a telemedicine app calls will be connected to an appointment. Using your own appointment id as the call id makes it easy to find the call later.


const callType = "default";
const callId = "test-call";
const call = client.call(callType, callId);
await call.getOrCreate();
// or create it with options:
await call.getOrCreate({
  data: {
    /* call creation options */
  },
});
See all possible options at the Call creation options section.

Join call

const callType = "default";
const callId = "test-call";
const call = client.call(callType, callId);
await call.join();
Create and join a call
For convenience, you can create and join a call in a single operation. One of the flags you can provide there is create. Set this to true if you want to enable creating new call. Set it to false if you only want to join an existing call.

See all possible options at the Call creation options section.


await call.join({
  create: true,
  data: {
    /* call creation options */
  },
});
Leave call
To leave a call, you can use the leave method:


await call.leave();
End call
Ending a call requires a special permission. This action terminates the call for everyone.


await call.endCall();
Only users with a special permission (OwnCapability.JOIN_ENDED_CALL) can join an ended call.

Load call
Existing calls can be loaded through the following API:


const call = client.call(type, id);
await call.get(); // just load
await call.getOrCreate(); // create if not present and load it
These operations initialize the call.state and create a subscription for call updates to our backend. This means that this call instance will receive real-time updates in case it is modified somewhere else.

Read more about call state here: Call & Participant State.

Update call
After creating a call, you can update some of its properties:


import { RecordSettingsRequestModeEnum } from "@stream-io/video-react-sdk";
await call.update({
  custom: { color: "green" },
  settings_override: {
    recording: {
      mode: RecordSettingsRequestModeEnum.DISABLED,
    },
  },
});
Call creation options
The following options are supported when creating a call:

Option	Description	Default
members	A list of members to add to this call. You can specify the role and custom data on these members	-
custom	Any custom data you want to store	-
settings	You can overwrite certain call settings for this specific call. This overwrites the call type standard settings	-
startsAt	When the call will start. Used for calls scheduled in the future, livestreams, audio rooms etc	-
team	Restrict the access to this call to a specific team	-
ring	If you want the call to ring for each member	false
notify	If you want the call to nofiy each member by sending push notification.	false
video	When ringing, the notification will indicate whether it’s a video call or an audio-only call, depending on whether you set the video parameter to true or false	-
Set call members

const call = client.call(type, id);
await call.getOrCreate({
  data: {
    members: [{ user_id: "alice", role: "admin" }, { user_id: "bob" }],
  },
});
Update call members

await call.updateCallMembers({
  update_members: [{ user_id: "charlie", role: "admin" }],
  remove_members: ["alice"],
});
Multi-tenant & teams
In a multi-tenant application, calls created by a user who is part of a team must specify the same team, or the request will be rejected with an error:


const call = client.call(type, id);
await call.getOrCreate({
  data: { team: "red" },
});
Keep in mind that you still need to enforce that call IDs are unique. Two common ways to do this are to generate IDs using UUIDs, or to prefix the call ID with the team name.

Custom call data

await call.getOrCreate({
  data: {
    custom: { color: "blue" },
  },
});
Settings override
By default, the call instances inherit the settings defined in the call type. In some cases, you might want to override call settings on the instance itself:


// at creation time
await call.getOrCreate({
  data: {
    settings_override: {
      audio: { mic_default_on: false },
      video: { camera_default_on: false },
    },
  },
});
// or later
await call.update({
  settings_override: {
    video: { camera_default_on: true },
  },
});
Backstage setup
The backstage feature makes it easy to build a use-case where you and your co-hosts can set up your camera before going live. Only after you call call.goLive() the regular users will be allowed to join the livestream.

However, you can also specify a join_ahead_time_seconds, which will allow regular users to join the livestream before the call is live, in the specified join time before the stream starts.

Here’s an example of how to do that:


await call.getOrCreate({
  data: {
    starts_at: new Date(Date.now() + 500 * 1000), // 500 seconds from now
    settings_override: {
      backstage: {
        enabled: true,
        join_ahead_time_seconds: 300,
      },
    },
  },
});
In the code snippet above, we are creating a call that starts 500 seconds from now. We are also enabling backstage mode, with a join_ahead_time_seconds of 300 seconds. That means that regular users will be able to join the call 200 seconds from now.

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Client & Authentication
Next
Call & Participant State
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Call & Participant State
Call & Participant State
You can access call, participant, and client state using hooks. These hooks are reactive (their value is updated on WebSocket events and API calls).

Call state
To observe call state, you need to provide a Call instance to the StreamCall component.

For the best experience, please make sure that the provided Call instance is loaded and connected to our backend: Load Call.

Otherwise, call.state and the call state hooks will provide empty values.

Let’s see an example where we use the useCall, useCallCallingState and useParticipants hooks to display some basic information about the call:


import {
  Call,
  StreamCall,
  useCall,
  useCallStateHooks,
} from "@stream-io/video-react-native-sdk";
export default function MyApp() {
  let call: Call;
  return (
    <StreamCall call={call}>
      <MyCallUI />
    </StreamCall>
  );
}
const MyCallUI = () => {
  const call = useCall();
  const { useCallCallingState, useParticipants } = useCallStateHooks();
  const callingState = useCallCallingState();
  const participants = useParticipants();
  return (
    <View>
      <Text>Call: {call?.cid}</Text>
      <Text>State: {callingState}</Text>
      <Text>Participants: {participants.length}</Text>
    </View>
  );
};
This approach makes it possible to access the call state and be notified about changes anywhere in your application without having to manually subscribe to WebSocket events.

The StreamCall component is a context provider that makes the call state available to all child components. The useCall hook returns the Call instance that is registered with StreamCall. You need the Call instance to initiate API calls.

Call State Hooks
Here is an excerpt of the available call state hooks:

Name	Description
useCall	The Call instance that is registered with StreamCall. You need the Call instance to initiate API calls.
useCallBlockedUserIds	The list of blocked user IDs.
useCallCallingState	Provides information about the call state. For example, RINGING, JOINED or RECONNECTING.
useCallClosedCaptions	The closed captions of the call.
useCallCreatedAt	The time the call was created.
useCallCreatedBy	The user that created the call.
useCallCustomData	The custom data attached to the call.
useCallEgress	The egress information of the call.
useCallEndedAt	The time the call was ended.
useCallEndedBy	The user that ended the call.
useCallIngress	The ingress information of the call.
useCallMembers	The list of call members
useCallSession	The information for the current call session.
useCallSettings	The settings of the call.
useCallStartedAt	The actual start time of the current call session.
useCallStartsAt	The scheduled start time of the call.
useCallStatsReport	When stats gathering is enabled, this observable will emit a new value at a regular (configurable) interval.
useCallThumbnail	The thumbnail of the call.
useCallUpdatedAt	The time the call was last updated.
useCameraState	The camera state of the local participant.
useDominantSpeaker	The participant that is the current dominant speaker of the call.
useHasOngoingScreenShare	It will return true if at least one participant is sharing their screen.
useHasPermissions	Returns true if the local participant has all the given permissions.
useIncomingVideoSettings	The state of manual overrides to incoming video quality.
useIsCallCaptioningInProgress	It’s true if the call is being close-captioned.
useIsCallHLSBroadcastingInProgress	It’s true if the call is being broadcasted in HLS mode.
useIsCallLive	It’s true if the call is currently live.
useIsCallRecordingInProgress	It’s true if the call is being recorded.
useIsCallTranscribingInProgress	It’s true if the call is being transcribed.
useMicrophoneState	The microphone state of the local participant.
useOwnCapabilities	The capabilities of the local participant.
useScreenShareState	The screen share state of the local participant.
useSpeakerState	Not supported in React Native
In your IDE of choice, you can see the full list if you destructure the useCallStateHooks object:


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const {
  useCallMembers,
  useDominantSpeaker,
  useParticipants,
  useLocalParticipant,
  useIsCallRecordingInProgress,
  // ...
} = useCallStateHooks();
Participant state
If you want to display information about the joined participants of the call, you can use the following hooks. The participant hooks return a StreamVideoParticipant object that contains information about the participant.

Participant State Hooks
Name	Description
useAnonymousParticipantCount	The approximate participant count of anonymous users in the active call.
useLocalParticipant	The local participant is the logged-in user.
useParticipantCount	The approximate participant count of the active call. This includes the anonymous users as well, it is computed on the server-side.
useParticipants	All participants, including local and remote participants.
usePinnedParticipants	The participants that are currently pinned.
useRawParticipants	A version of useParticipants that is not affected by participant sort settings and thus causes less component updates.
useRemoteParticipants	All participants except the local participant.
Warning: In a call with many participants, the value of the useParticipants() is truncated to 250 participants.

The participants who are publishing video, audio, or screen sharing have priority over the other participants in the list. This means, for example, that in a livestream with one host and many viewers, the host is guaranteed to be in the list.


import {
  Call,
  useCallStateHooks,
  StreamCall,
} from "@stream-io/video-react-native-sdk";
export default function App() {
  let call: Call;
  return (
    <StreamCall call={call}>
      <MyCallUI />
    </StreamCall>
  );
}
const MyCallUI = () => {
  const { useLocalParticipant, useParticipantCount } = useCallStateHooks();
  const participantCount = useParticipantCount();
  const localParticipant = useLocalParticipant();
  return (
    <View>
      <Text>Number of participants: {participantCount}</Text>
      <Text>Session ID: {localParticipant.sessionId}</Text>
    </View>
  );
};
Participant data
The StreamVideoParticipant object has the following properties:

Name	Description
audioLevel	The audio level of the participant (determined on the server).
audioStream	The audio MediaStream.
audioVolume	The audio volume level of the participant (overridable local audioVolume level).
connectionQuality	The participant’s connection quality.
custom	The participant’s custom data. Comes from the custom field of the user object.
image	The image of the participant.
isDominantSpeaker	It’s true if the participant is the current dominant speaker in the call.
isLocalParticipant	It’s true if the participant is the local participant.
isSpeaking	It’s true if the participant is currently speaking.
joinedAt	The time the participant joined the call.
name	The name of the participant.
pausedTracks	The tracks that are currently server-side paused for the local participant.
pin	Holds pinning information.
publishedTracks	The track types the participant is currently publishing
reaction	The last reaction this user has sent to this call.
roles	The roles of the participant in this call.
screenShareAudioStream	The screen share audio MediaStream.
screenShareStream	The screen share MediaStream.
sessionId	The identifier of the participant within the existing call session
source	The participant source: WebRTC (default), RTMP (OBS), WHIP, SIP, RTSP, SRT…
userId	The user ID of the participant.
videoStream	The video MediaStream.
viewportVisibilityState	The viewport visibility state of the participant.
Utility functions
The SDK also provides a few utility functions that help you to work with participants:


import {
  hasAudio,
  hasVideo,
  hasScreenShare,
  hasScreenShareAudio,
  hasPausedTrack,
  isPinned,
  useCallStateHooks,
} from "@stream-io/video-react-native-sdk";
// example usage
const { useParticipants } = useCallStateHooks();
const participants = useParticipants();
// check if the participant has audio, video, screen share or screen share audio
const [participant] = participants;
const hasAudioOn = hasAudio(participant);
const hasVideoOn = hasVideo(participant);
const hasScreenShareOn = hasScreenShare(participant);
const hasScreenShareAudioOn = hasScreenShareAudio(participant);
const isPinnedOn = isPinned(participant);
const isVideoPaused = hasPausedTrack(participant, "videoTrack");
// participants with a specific role
const hosts = participants.filter((p) => p.roles.includes("host"));
// participants that publish video and audio
const videoParticipants = participants.filter(
  (p) => hasVideo(p) && hasAudio(p),
);
Detecting participant source
Participants can be created from different sources (WebRTC, RTMP/OBS, WHIP, SIP, etc…). The source property of the StreamVideoParticipant object indicates the source of the participant.


import { SfuModels } from "@stream-io/video-react-native-sdk";
const { useParticipants } = useCallStateHooks();
const participants = useParticipants();
// participants joining through OBS have RTMP source
const rtmpParticipants = participants.filter(
  (p) => p.source === SfuModels.ParticipantSource.RTMP,
);
Client state
To observe client state you need to provide a StreamVideoClient instance to the StreamVideo context provider. If you want to observe the connected user you can use the useConnectedUser hook.

Let’s see an example:


import {
  useConnectedUser,
  StreamVideo,
  StreamVideoClient,
} from "@stream-io/video-react-native-sdk";
export default function App() {
  let client: StreamVideoClient;
  return (
    <StreamVideo client={client}>
      <MyHeader />
    </StreamVideo>
  );
}
const MyHeader = () => {
  const user = useConnectedUser();
  return <Text>{user ? `Logged in: ${user.name}` : "Logged out"}</Text>;
};
This approach makes it possible to access the client state and be notified about changes anywhere in your application without having to manually subscribe to WebSocket events.

Client state hooks
Name	Description
useStreamVideoClient	The StreamVideoClient instance.
useConnectedUser	Returns the connected user. Holds the server-side data of the connected user.
useCalls	A list of all tracked calls. These calls can be outgoing (I have called somebody) or incoming (somebody has called me). Loaded calls (call.get()) are also part of this list.
The connectedUser object contains the following properties:

Name	Description
created_at	The time the user was created.
custom	Custom user data.
deleted_at	The time the user was deleted.
devices	The registered push notification devices of the user.
id	The id of the user.
image	The profile image of the user.
name	The name of the user.
role	The role of the user.
teams	The teams the user belongs to.
updated_at	The time when the user was updated.
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Joining & Creating Calls
Next
Calling State and Lifecycle
© Getstream.io, Inc. All Rights Reserved.   ,  Video and Audio
/
Docs
/
React Native
/
Calling State and Lifecycle
Calling State and Lifecycle
The call object instance manages everything related to a particular call instance, such as:

creating and joining a call
performing actions (mute, unmute, send reaction, etc…)
manage event subscriptions (call.on('call.session_started', callback), etc…)
and many more
Every call instance should be created through the client.call(type, id) helper.

Our StreamVideoClient is responsible for maintaining a WebSocket connection to our servers and also takes care about the API calls that are proxied from the call instance.

As we learned in Joining and Creating Calls guide, a call instance is managed like this:


import { Call, StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient; // ...
const call: Call = client.call(type, id);
// load existing call information from our servers
await call.get();
// Creates the call on our servers in case it doesn't exist. Otherwise,
// loads the call information from our servers.
await call.getOrCreate();
// join the call
await call.join();
// leave the call and dispose all allocated resources
await call.leave();
Every call instance has a local state, exposed to integrators through:

call.state.callingState - a getter that returns the current value
call.state.callingState$ - an observable that an integrator can subscribe to and be notified everytime the value changes
useCallCallingState() - a call state hook that makes it easy to read and update the UI based on calling state values in React components.
Call Instance
The call instance is a stateful resource that you acquire with client.call() and must dispose of with call.leave(). Failure to dispose of the call properly can result in memory leaks and unexpected behavior.

In practice, this means that:

You should only create call instances in effects.
Effects that create a call instance should have a call.leave() cleanup.

const [call, setCall] = useState<Call | undefined>(undefined);
useEffect(() => {
  const myCall = client.call(callType, callId);
  myCall.join({ create: true }).then(
    () => setCall(myCall),
    () => console.error("Failed to join the call"),
  );
  return () => {
    myCall.leave().catch(() => console.error("Failed to leave the call"));
    setCall(undefined);
  };
}, [callType, callId]);
To join the same call again, you can reuse the same call instance, or create a new one using client.call(type, id).

Calling State
Every call instance has its own local state managed by the SDK.

These values are exposed through the CallingState enum:


import {
  CallingState,
  useCallStateHooks,
} from "@stream-io/video-react-native-sdk";
const { useCallCallingState } = useCallStateHooks();
const callingState = useCallCallingState();
switch (callingState) {
  case CallingState.JOINED:
    // ...
    break;
  default:
    const exhaustiveCheck: never = callingState;
    throw new Error(`Unknown calling state: ${exhaustiveCheck}`);
}
As CallingState is an enum that can be extended at any time by us, it would be good to make sure you use it exhaustively. This way, if you use TypeScript, you can get a compile time error and be notified that there are few more states that you should handle.

Calling States
State	Description
CallingState.UNKNOWN	The state is unknown. This value is set when Calling State isn’t initialized properly.
CallingState.IDLE	A call instance is created on the client side but a WebRTC session isn’t established yet.
CallingState.RINGING	This is an incoming (ring) call. You are the callee.
CallingState.JOINING	The call join flow is executing (typically right after call.join()). Our systems are preparing to accept the new call participant.
CallingState.JOINED	The join flow has finished successfully and the current participant is part of the call. The participant can receive and publish audio and video.
CallingState.LEFT	The call has been left (call.leave()) and all allocated resources are released. Please create a new call instance if you want to re-join.
CallingState.RECONNECTING	A network connection has been lost (due to various factors) and the call instance attempts to re-establish a connection and resume the call.
CallingState.RECONNECTING_FAILED	The SDK failed to recover the connection after a couple of consecutive attempts. You need to inform the user that he needs to go online and manually attempt to rejoin the call.
CallingState.MIGRATING	The SFU node that is hosting the current participant is shutting down or tries to rebalance the load. This call instance is being migrated to another SFU node.
CallingState.OFFLINE	No network connection can be detected. Once the connection restores, the SDK will automatically attempt to recover the connection (signalled with RECONNECTING state).
Example handling
To understand these values better, here is a hypothetical example of how these values can be mapped:


import {
  CallingState,
  useCallStateHooks,
  CallContent,
} from "@stream-io/video-react-native-sdk";
const call = useCall();
const isCallCreatedByMe = call?.isCreatedByMe;
const { useCallCallingState } = useCallStateHooks();
const callingState = useCallCallingState();
switch (callingState) {
  case CallingState.RINGING:
    return isCallCreatedByMe ? (
      <OutgoingCallFullScreenComponent />
    ) : (
      <IncomingCallFullScreenComponent />
    );
  case CallingState.LEFT:
    return <CallLeftIndicatorFullScreenComponent />;
  case CallingState.IDLE:
    return <CallPreparingFullScreenComponent />;
  default:
    return <CallContent />;
}
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Call & Participant State
Next
Camera & Microphone
© Getstream.io, Inc. All Rights Reserved.  , Video and Audio
/
Docs
/
React Native
/
Camera & Microphone
Camera & Microphone
Handling audio and video devices in your application means working with MediaStream, MediaDeviceInfo and other WebRTC API objects. To simplify this, we hide all the complexity inside the SDK and export utility functions and states. In this guide, we shall go over their usage.

Camera management
The SDK does its best to make working with the camera easy. We expose the following camera object on the call:


const call = useCall();
const camera = call.camera;
Call settings
The default state of the camera is determined by the settings in the call object.


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useCallSettings } = useCallStateHooks();
const settings = useCallSettings();
console.log(settings?.video.camera_default_on);
Make sure, call.get() is called at least once in the application, after the call is created.

Start-Stop Camera
We can use the functions camera.enable() and camera.disable() to control the publishing and unpublishing our video stream.

Alternatively, you can use camera.toggle().


import { useCall, useCallStateHooks } from "@stream-io/video-react-native-sdk";
const call = useCall();
const { useCameraState } = useCallStateHooks();
const { camera, isMute } = useCameraState();
console.log(`Camera is ${isMute ? "off" : "on"}`);
await camera.toggle();
// or, alternatively
await camera.enable();
await camera.disable();
It’s always best to await calls to enable(), disable(), and toggle(), however the SDK does its best to resolve potential race conditions: the last call always wins, so it’s safe to make these calls in an event handler.

The status is updated once the camera is actually enabled or disabled. Use optimisticIsMute for the “optimistic” status that is updated immediately after toggling the camera.

Manage Camera Facing Mode
We can toggle the camera face from front to back and vice versa using camera.flip().


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useCameraState } = useCallStateHooks();
const { camera } = useCameraState();
console.log(direction); // direction returns 'front' or 'back'.
camera.flip();
We can get the facing mode state of the camera by:


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useCameraState } = useCallStateHooks();
const { direction } = useCameraState(); // direction returns 'front' or 'back'.
Video mute status
We can get the mute state of our video stream by checking the status value returned from the useCameraState hook:


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useCameraState } = useCallStateHooks();
const { status } = useCameraState(); // status returns enabled, disabled or undefined
Show Video Preview
We can get the video stream from the camera using the media stream from the call.camera object and show it using the RTCView component from @stream-io/react-native-webrtc library:


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
import { RTCView } from "@stream-io/react-native-webrtc";
const { useCameraState } = useCallStateHooks();
const { camera } = useCameraState();
const localVideoStream = camera.state.mediaStream;
return <RTCView streamURL={localVideoStream?.toURL()} />;
Access to the Camera’s MediaStream
Our SDK exposes the current mediaStream instance that you can use for your needs (for example, local recording, etc…):


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useCameraState } = useCallStateHooks();
const { mediaStream } = useCameraState();
const [videoTrack] = mediaStream.getVideoTracks();
console.log("Video track", videoTrack);
Microphone management
The SDK does its best to make working with the microphone easy. We expose the following microphone object on the call:


const call = useCall();
const microphone = call.microphone;
Call settings
The default state of the microphone is determined by the settings in the call object.


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useCallSettings } = useCallStateHooks();
const settings = useCallSettings();
console.log(settings?.audio.mic_default_on);
Make sure, call.get() is called at least once in the application, after the call is created.

Start-Stop Microphone
We can use the functions microphone.enable() and microphone.disable() to control the publishing and unpublishing our audio stream:

Alternatively, you can use microphone.toggle().


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useMicrophoneState } = useCallStateHooks();
const { microphone, isMute } = useMicrophoneState();
console.log(`Microphone is ${isMute ? "off" : "on"}`);
await microphone.toggle();
// or, alternatively
await microphone.enable();
await microphone.disable();
It’s always best to await calls to enable(), disable(), and toggle(), however the SDK does its best to resolve potential race conditions: the last call always wins, so it’s safe to make these calls in an event handler.

The status is updated once the microphone is actually enabled or disabled. Use optimisticIsMute for the “optimistic” status that is updated immediately after toggling the microphone.

Audio mute status
We can get the mute state of our audio stream by checking the status value returned from the useMicrophoneState hook:


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useMicrophoneState } = useCallStateHooks();
const { status } = useMicrophoneState(); // status returns enabled, disabled or undefined
Speaking while muted detection
Our SDK provides a mechanism that can detect whether the user started to speak while being muted. Through this mechanism, you can display a notification to the user, or apply any custom logic.

This feature is enabled by default unless the user doesn’t have the permission to send audio or explicitly disabled.


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useMicrophoneState } = useCallStateHooks();
const { isSpeakingWhileMuted, microphone } = useMicrophoneState();
if (isSpeakingWhileMuted) {
  // your custom logic comes here
  console.log("You are speaking while muted!");
}
// to disable this feature completely:
await microphone.disableSpeakingWhileMutedNotification();
// to enable it back:
await microphone.enableSpeakingWhileMutedNotification();
Access to the Microphone’s MediaStream
Our SDK exposes the current mediaStream instance that you can use for your needs (for example, local recording, etc…):


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useMicrophoneState } = useCallStateHooks();
const { mediaStream } = useMicrophoneState();
const [audioTrack] = mediaStream.getAudioTracks();
console.log("Audio track", audioTrack);
Speaker management
We can use the callManager module from the SDK for managing speaker audio output in our SDK. The start() and stop() methods are to be used for managing audio. Before joining a call or as soon as you join a call, call the start() method. Once you leave the call, use the stop() method.


import { callManager } from "@stream-io/video-react-native-sdk";
// To be called before joining a call or as soon as joining a call
callManager.start({
  audioRole: "communicator", // or "listener"
  deviceEndpointType: "speaker", // or "earpiece"
});
// To be called when a call is left
callManager.stop();
audioRole: communicator (default) or listener. Use listener for users that won’t publish audio but only listen (typical for livestream audience). Use communicator otherwise.

deviceEndpointType: speaker or earpiece. Available only when audioRole is set to communicator. speaker enables the loudspeaker when no bluetooth device or other wired headset is connected. earpiece routes the audio through the ear-speaker unless another external device is connected. earpiece should be passed only on a scenario of an audio call similar to a mobile cellular phone call.

The following device priority is used:

Bluetooth Headset or Wired Headset
Speakerphone or Earpiece.
As platform-specific methods are necessary to handle audio output, we do not support the useSpeakerState() hook.

Switching audio output device
The API for switching the audio output device is vastly different on iOS and Android.


iOS

Android
In iOS, use the following method to open the popover from AVRoutePickerView that is presented by the system. The popover will show the audio devices that are available, and the user can tap on the list to choose the desired audio device.


import { callManager } from "@stream-io/video-react-native-sdk";
callManager.ios.showDeviceSelector();
Once this method is called, a popover similar to the on below will appear.

Preview of the iOS audio route picker
Force audio through the loudspeaker
A common use case is to allow switching the audio through the load speaker and ear speaker, e.g.: through a button toggle. To support this behavior, you can use the following method on both iOS and Android:


import { callManager } from "@stream-io/video-react-native-sdk";
// route audio through loud speaker immediately (audio outputs here until a new external device is connected)
callManager.speaker.setForceSpeakerphoneOn(true);
// revert back to default behaviour
callManager.speaker.setForceSpeakerphoneOn(false);
Audio volume control
The SDK supports both system-wide audio volume control and individual track level audio volume control.

System wide mute and unmute of audio volume

import { callManager } from "@stream-io/video-react-native-sdk";
// to mute audio
callManager.setMute(true);
// to unmute audio
callManager.setMute(false);
Participant volume control
We also support setting a participant audio volume. Here is a small snippet of how to set a participant volume to 50%:


import { type StreamVideoParticipant } from "@stream-io/video-react-native-sdk";
let participant: StreamVideoParticipant; // the intended participant
call.speaker.setParticipantVolume(participant.sessionId, 0.5);
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Calling State and Lifecycle
Next
Noise Cancellation
© Getstream.io, Inc. All Rights Reserved.  , Video and Audio
/
Docs
/
React Native
/
Noise Cancellation
Noise Cancellation
The Noise Cancellation feature in our Stream Video React Native SDK can be enabled by installing the @stream-io/noise-cancellation-react-native package to your project and by having it enabled in the Stream dashboard. This package leverages noise cancellation technology developed by krisp.ai.

Installation
To enable noise cancellation feature in your app. You must first add the @stream-io/noise-cancellation-react-native library. This library adds the required native module for processing the audio stream and manipulating it with the noise cancellation filter.


Expo

React Native CLI
Terminal
npx expo install @stream-io/noise-cancellation-react-native
Noise cancellation is supported only on 125.3.0 version and above of @stream-io/react-native-webrtc. Expo version 50 or above is required.

Add the config plugin properties
In app.json, in the plugins field, add the addNoiseCancellation property to the @stream-io/video-react-native-sdk plugin.

app.json
{
  "plugins": [
    [
      "@stream-io/video-react-native-sdk",
      {
        "addNoiseCancellation": true
        // ... any other props
      }
    ]
    // your other plugins
  ]
}
The addNoiseCancellation field is used to add the relevant native code to initialise the noise cancellation audio filter on iOS and Android.
If Expo EAS build is not used, please do npx expo prebuild --clean to generate the native directories again after adding the config plugins.

Integration
Our React Native SDK provides utility components and hooks that make the integration smoother.

NoiseCancellationProvider - a context provider component. This component must be a child of the StreamCall component.
useNoiseCancellation() - a hook that exposes the API that controls the NoiseCancellation behavior

import { Button } from "react-native";
import {
  Call,
  NoiseCancellationProvider,
  StreamCall,
  StreamVideo,
  StreamVideoClient,
  useNoiseCancellation,
} from "@stream-io/video-react-native-sdk";
export const MyApp = () => {
  let client: StreamVideoClient; // ...
  let call: Call; // ...
  return (
    <StreamVideo client={client}>
      <StreamCall call={call}>
        <NoiseCancellationProvider>
          <MyComponentTree>
            <MyToggleNoiseCancellationButton />
          </MyComponentTree>
        </NoiseCancellationProvider>
      </StreamCall>
    </StreamVideo>
  );
};
// a minimal example of a noise cancellation button
const MyToggleNoiseCancellationButton = () => {
  const {
    // `isSupported` can be true, false or undefined. It is   false when the server settings indicate that user must not not turn noise cancellation on. It is `undefined` while user capability check is in progress.
    isSupported,
    // `deviceSupportsAdvancedAudioProcessing` can be true, false or undefined. It is `true` when Apple's Neural Engine can be used or AUDIO_PRO on Android. It is `undefined` used while device capability check is in progress.
    deviceSupportsAdvancedAudioProcessing,
    isEnabled,
    setEnabled,
  } = useNoiseCancellation();
  if (!deviceSupportsAdvancedAudioProcessing) {
    // optional but recommended
    // do not show the option if the device does support advanced audio processing.
    return null;
  }
  return (
    <Button
      disabled={!isSupported}
      title={
        isEnabled ? "Disable Noise Cancellation" : "Enable Noise Cancellation"
      }
      onPress={() => setEnabled((prev) => !prev)}
    />
  );
};
While noise cancellation may be enabled, it is a resource-intensive process. It is recommended to enable it only on devices that support advanced audio processing.

You can check if a device supports advanced audio processing with the deviceSupportsAdvancedAudioProcessing boolean value from the useNoiseCancellation hook.

This method returns true if the iOS device supports Apple’s Neural Engine or if an Android device has the FEATURE_AUDIO_PRO feature enabled. Devices with this capability are better suited for handling noise cancellation efficiently.

Feature availability
The feature availablity is automatically handled by the NoiseCancellationProvider component based on the call settings. When the settings are configured that noise cancellation should not be enabled, the isSupported boolean value from the useNoiseCancellation hook will be false. And vice versa. Also, the noise cancellation settings can also be accessed using the Call State Hooks. The noise_cancellation setting contains a mode property that indicates availability:


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useCallSettings } = useCallStateHooks();
const settings = useCallSettings();
// "available", "disabled", "auto-on"
console.log(settings?.audio.noise_cancellation?.mode);
available The feature has been enabled on the dashboard and it’s available for the call. In this case, you are free to present any noise cancellation toggle UI in your application.

disabled The feature hasn’t been enabled on the dashboard or the feature isn’t available for the call. In this case, you should hide any noise cancellation toggle UI in your application.

auto-on Similar to available with the difference that if possible, the SDK will enable the noise cancellation automatically, when the user join the call and if the deviceSupportsAdvancedAudioProcessing value from the useNoiseCancellation hook is true.

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Camera & Microphone
Next
Call Types
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Call Types
Call Types
The Video SDK uses a set of pre-defined call types that come with different default permissions and feature configurations. Depending on your use case you can also extend those and use custom types that suit your needs.

Let’s start with some clarification on naming.

Call Type: 4 default call types come with a set of pre-defined user roles and capabilities that are assigned to these roles. These default types can be used but it’s also possible to define custom call types via the dashboard.

User Role: Users can have different roles (note: one user can have multiple roles). Again, there are pre-defined user roles that each come with a certain set of capabilities. You can use the existing user roles or define custom ones via the dashboard.

Call Capabilities: Each participant of a call has certain capabilities (such as send-video or end-call). These are associated with a certain user role. The associations can be found further below and can also be customized via the dashboard.

Call Types
There are 4 pre-defined call types, these are:

default: simple 1-1 calls for larger group video calling with sensible defaults
audio_room: pre-configured for a workflow around requesting permissions in audio settings (speaking, etc.)
livestream: access to calls is granted to all authenticated users, useful in one-to-many settings (such as livestreaming)
development: should only be used for testing, permissions are open and everything is enabled (use carefully)
Before we go into more detail about what the different call types are for, let’s take a look at some of the concrete permissions and settings that each call type comes with.

Each call type comes with a set of settings. One important concept is called backstage. It means that calls can be created but not directly joined. That means you can schedule a call. It would then have backstage enabled until you call goLive() with it.

Now, let’s take a look at each of the call types in more detail.

Development
The development call type has all the permissions enabled and can be used during development. It’s not recommended to use this call type in production, since all the participants in the calls would be able to do everything (blocking, muting everyone, etc).

For these call types, backstage is not enabled, therefore you don’t have to explicitly call goLive for the call to be started.

Default
The default call type can be used for different video-calling apps, such as 1-1 calls, group calls, or meetings with multiple people. Both video and audio are enabled, and backstage is disabled. It has permissions settings in place, where admins and hosts have elevated permissions over other types of users.

The default type can be used in apps that use regular video calling. To learn more try our tutorial on building a video calling app.

Audio Room
The audio_room call type is suitable for apps like Clubhouse or Twitter Spaces. It has a pre-configured workflow around requesting permissions to speak for regular listeners. Backstage is enabled, and new calls are going into backstage mode when created. You will need to explicitly call the goLive method to make the call active for all participants.

You can find out how to handle this and build an application with our Audio Room tutorial.

Livestream
The livestream call type is configured to be used for live streaming apps. Access to calls is granted to all authenticated users, and backstage is enabled by default.

To build an example application for this you can take a look at our live streaming tutorial.

Call type settings
Each call comes with a number of settings. Depending on the type of call these are enabled or disabled.

You can see a full table of which call type has which setting enabled in the next chapter.

First, we’ll describe the different settings that exist in different categories.

Audio
Setting Name	Type	Description
access_request_enabled	Boolean	When true users that do not have permission to this feature can request access for it
opus_dtx_enabled	Boolean	When true OPUS DTX is enabled
redundant_coding_enabled	Boolean	When true redundant audio transmission is enabled
mic_default_on	Boolean	When true the user will join with the microphone enabled by default
speaker_default_on	Boolean	When true the user will join with the audio turned on by default
default_device	String speaker or earpiece	The default audio device to use
Backstage
Setting Name	Type	Description
enabled	Boolean	When backstage is enabled, calls will be in backstage mode when created and can be joined by users only after goLive is called
Video
Setting Name	Type	Description
enabled	Boolean	Defines whether video is enabled for the call
access_request_enabled	Boolean	When true users that do not have permission to this feature can request access for it
camera_default_on	Boolean	When true, the camera will be turned on when joining the call
camera_facing	String front, back or external	When applicable, the camera that should be used by default
target_resolution	Target Resolution Object	The ideal resolution that video publishers should send
The target resolution object is an advanced resolution. Changing this from the default values can lead to poor performance. This is how you define it:

Setting Name	Type	Description
width	Number	The width in pixels
height	Number	The height in pixels
bitrate	Number	The bitrate
Screensharing
Setting Name	Type	Description
enabled	Boolean	Defines whether screensharing is enabled
access_request_enabled	Boolean	When true users that do not have permission to this feature can request access for it
Recording
Setting Name	Type	Description
mode	String available, disabled or auto-on	available → recording can be requested
disabled → recording is disabled
auto-on → recording starts and stops automatically when one or multiple users join the call
quality	String audio-only, 360p, 480p, 720p, 1080p, 1440p	Defines the resolution of the recording
audio_only	boolean	If true the recordings will only contain audio
layout	object, for more information see the API docs	Configuration options for the recording application
Broadcasting
Setting Name	Type	Description
enabled	Boolean	Defines whether broadcasting is enabled
hls	HLS Settings (object)	Settings for HLS broadcasting
HLS Settings
Setting Name	Type	Description
enabled	Boolean	Defines whether HLS is enabled or not
auto_on	Boolean	When true HLS streaming will start as soon as users join the call
quality_tracks	String audio-only, 360p, 480p, 720p, 1080p, 1440p	The tracks to publish for the HLS stream (up to three tracks)
Geofencing
Setting Name	Type	Description
names	List of one or more of these strings european_union, iran_north_korea_syria_exclusion, china_exclusion, russia_exclusion, belarus_exclusion, india, united_states, canada	The list of geofences that are used for the calls of these type
More information can be found in the API docs.

Transcription
Setting Name	Type	Description
mode	String available, disabled or auto-on	Not implemented yet
closed_caption_mode	String	Not implemented yet
Ringing
Setting Name	Type	Description
incoming_call_timeout_ms	Number	Defines how long the SDK should display the incoming call screen before discarding the call (in ms)
auto_cancel_timeout_ms	Number	Defines how long the caller should wait for others to accept the call before canceling (in ms)
Push Notifications Settings
Setting Name	Type	Description
enabled	Boolean	
call_live_started	Event Notification Settings Object	The notification settings used for call_live_started events
session_started	Event Notification Settings Object	The notification settings used for session_started events
call_notification	Event Notification Settings Object	The notification settings used for call_notification events
call_ring	Event Notification Settings Object	The notification settings used for call_ring events
In order to define the event notification settings object, here is the structure of how it should look:

Setting Name	Type	Description
enabled	Boolean	Whether this object is enabled
apns	APNS Settings Object	The settings for APN notifications
APNS Settings Object
Remote notifications can only be customized if your application implements a Notification Service Extension. For simple customizations, you can change the title and body fields at the call type level. Both title and body fields are handlebars templates with call and user objects available in their scope.

Setting Name	Type	Description
title	Template	The string template for the title field of the notification
body	Template	The string template for the body field of the notification
Defaults for call type settings
audio-room	default	livestream	development
Audio				
access_request_enabled	✅	✅	❌	✅
opus_dtx_enabled	✅	✅	✅	✅
redundant_coding_enabled	✅	✅	✅	✅
mic_default_on	❌	✅	❌	✅
speaker_default_on	✅	✅	✅	✅
default_device	speaker	earpiece	speaker	earpiece
Backstage				
enabled	✅	❌	✅	❌
Video				
enabled	❌	✅	✅	✅
access_request_enabled	❌	✅	❌	✅
target_resolution	N/A	Width: 2560
Height 1440
Bitrate 5000000	Width: 1920
Height: 1080
Bitrate 3000000	Width: 1920
Height 1080
Bitrate 3000000
camera_default_on	❌	✅	✅	✅
camera_facing	front	front	front	front
Screensharing				
enabled	❌	✅	✅	✅
access_request_enabled	❌	✅	❌	✅
Recording				
mode	available	available	available	available
quality	720p	720p	720p	720p
Broadcasting				
enabled	✅	✅	✅	✅
hls.auto_on	❌	❌	❌	❌
hls.enabled	available	available	available	available
hls.quality_tracks	[720p]	[720p]	[720p]	[720p]
Geofencing				
names	[]	[]	[]	[]
Transcriptions				
mode	available	available	available	available
Ringing				
incoming_call_timeout_ms	0	15000	0	15000
auto_cancel_timeout_ms	0	15000	0	15000
User roles
There are 5 pre-defined user roles, these are:

user
moderator
host
admin
call-member
As mentioned before each user role is associated with a set of call capabilities. You can access the default roles and their capabilities in the Stream Dashboard.

In general, it makes sense to have a solid setup of roles as it makes handling permissions and requests easier.

Call Capabilities
A capability defines the actions that a certain user is allowed to perform on a call. There are many different available (see a full list in the next chapter). Each user has a certain set of capabilities attached to them. You can change these default capabilities in the dashboard. It is also possible to dynamically change these.

That means that if a user has permission to assign new capabilities they can assign them to other users. This is our approach to an effective permission system.

If you want to learn more about doing this, head over to the Permissions and Capabilities chapter.

Default call capabilities
When a call is fetched from the API by a user, the response includes the list of actions that the user is allowed to perform on the call.

These are the following:

join-call
read-call
create-call
join-ended-call
join-backstage
update-call
update-call-settings
screenshare
send-video
send-audio
start-record-call
stop-record-call
start-broadcast-call
stop-broadcast-call
end-call
mute-users
update-call-permissions
block-users
create-reaction
pin-for-everyone
remove-call-member
start-transcription-call
stop-transcription-call
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Noise Cancellation
Next
Querying Calls
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Querying Calls
Querying Calls
The Stream Video SDK allows you to query calls and watch them. This allows you to build apps that display feeds of calls with real-time updates (without joining them).

You can query calls based on built-in fields as well as any custom field you add to the calls. Multiple filters can be combined using AND, OR logical operators, each filter can use its comparison (equality, inequality, greater than, greater or equal, etc.).

You can use the StreamVideoClient to query for:

Upcoming calls
Calls that are currently live
Popular live streams / audio rooms with a link to the recording
Client API
You can query calls by using the client directly by using the following API:


const { calls } = await client.queryCalls({
  filter_conditions: { ...filters },
  sort: [...sortOptions],
  limit: 25,
  watch: true,
});
Filters
Filter expressions support multiple match criteria, and it’s also possible to combine filters. You can filter on the following fields:

Field	Description
id	The id for this call
cid	The cid for this call. IE: default:123
team	The team id for the call.
type	The call type. Typically default, livestream etc…
created_by_user_id	The user id who created the call
created_at	When the call was created
updated_at	When the call was updated
ended_at	When the call ended
starts_at	When the call starts at
backstage	If the call is in backstage mode or not
members	Check if the call has these members listed
ongoing	Check if the call is ongoing or not
custom	You can query custom data using the "custom.myfield" syntax
For more information, visit the filter operators guide. Or check the examples:

Calls that are about to start
In this snippet, you can see how you can query for calls that have livestream type and are about to start 30 minutes from now:


import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient;
const inNext30mins = new Date(Date.now() + 1000 * 60 * 60 * 30);
const { calls } = await client.queryCalls({
  filter_conditions: {
    type: { $eq: "livestream" },
    starts_at: { $gt: inNext30mins.toISOString() },
  },
  sort: [{ field: "starts_at", direction: -1 }],
  limit: 10,
  watch: true,
});
Call filters on a custom property

import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient;
const { calls } = await client.queryCalls({
  filter_conditions: { "custom.color": "red" },
  limit: 10,
  watch: true,
});
Calls that are ongoing / currently have participants

import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient;
const { calls } = await client.queryCalls({
  filter_conditions: { ongoing: true },
});
Calls the user has created or is a member of

import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient;
const { calls } = await client.queryCalls({
  filter_conditions: {
    $or: [
      { created_by_user_id: "<user id>" },
      { members: { $in: ["<user id>"] } },
    ],
  },
  limit: 10,
  watch: true,
});
Sorting
The SortParamRequest model contains two properties: field and direction.

The direction can be 1 for ascending and -1 for descending, while the field can be one of the following values:

Field	Description
starts_at	When the call starts at
created_at	When the call was created
updated_at	When the call was updated
ended_at	When the call ended
type	The call type. Typically default, livestream etc…
id	The id for this call
cid	The cid for this call. IE: default:123

import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient;
const { calls } = await client.queryCalls({
  sort: [{ field: "starts_at", direction: -1 }],
  limit: 10,
  watch: true,
});
It’s possible to provide multiple sort parameters:


import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient;
const { calls } = await client.queryCalls({
  sort: [
    { field: "starts_at", direction: -1 },
    { field: "created_at", direction: 1 },
  ],
  limit: 10,
  watch: true,
});
Watching calls
If you specify watch: true as an option, the SDK will create a subscription to the call data on the server and you’ll be able to receive updates in real-time.

The server will send updates to the client when the call data changes (for example, members are updated, a call session has started, etc…). This is useful for showing a live preview of who is in the call or building a call dashboard.

Pagination
You can specify the page size using the limit option. The API response will include links to the previous/next pages. The following code example shows how pagination works:


import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
let client: StreamVideoClient;
const inNext30mins = new Date(Date.now() + 1000 * 60 * 60 * 30);
const callQuery = {
  filter_conditions: {
    type: { $eq: "livestream" },
    starts_at: { $gt: inNext30mins.toISOString() },
  },
  sort: [{ field: "starts_at", direction: -1 }],
  limit: 10,
  watch: true,
};
let { calls, prev, next } = await client.queryCalls(callQuery);
// Go to the next page
({ calls, prev, next } = await client.queryCalls({ ...callQuery, next }));
// Go to the previous page
({ calls, prev, next } = await client.queryCalls({ ...callQuery, prev }));
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Call Types
Next
Querying Call Members
© Getstream.io, Inc. All Rights Reserved.    , Video and Audio
/
Docs
/
React Native
/
Querying Call Members
Querying Call Members
When you create or join a call you get a list of call members, however this can return at most 100 members:


// The maximum limit is 100
// The default limit is 25
await call.getOrCreate({ members_limit: 100 });
// or
await call.join({ members_limit: 100 });
To get the complete list of call members the Stream API allows you to query, filter and sort members of a call using a paginated list.

Examples
Below are a few examples of how to use this API:


const result = await call.queryMembers();
// sorting and pagination
const queryMembersReq = {
  sort: [{ field: "user_id", direction: 1 }],
  limit: 2,
};
const result = await call.queryMembers(queryMembersReq);
// loading the next page
const result = await call.queryMembers({
  ...queryMembersReq,
  next: result.next,
});
// filtering
const result = await call.queryMembers({
  filter_conditions: { role: { $eq: "admin" } },
});
Sort options
Sorting is supported on these fields:

user_id
created_at
Filter options
Name	Type	Description	Supported operators
user_id	string	User ID	$in, $eq, $gt, $gte, $lt, $lte, $exists
role	string	The role of the user	$in, $eq, $gt, $gte, $lt, $lte, $exists
custom	Object	Search in custom membership data, example syntax: {'custom.color': {$eq: 'red'}}	$in, $eq, $gt, $gte, $lt, $lte, $exists
created_at	string, must be formatted as an RFC3339 timestamp (for example 2021-01-15T09:30:20.45Z)	Creation time of the user	$in, $eq, $gt, $gte, $lt, $lte, $exists
updated_at	string, must be formatted as an RFC3339 timestamp (for example 2021-01-15T09:30:20.45Z)	The time of the last update of the user	$in, $eq, $gt, $gte, $lt, $lte, $exists
The Stream API allows you to specify filters and ordering for several endpoints. The query syntax is similar to that of Mongoose, however we do not run MongoDB on the backend. Only a subset of the MongoDB operations are supported.

Name	Description	Example
$eq	Matches values that are equal to a specified value.	{ "key": { "$eq": "value" } } or the simplest form { "key": "value" }
$q	Full text search (matches values where the whole text value matches the specified value)	{ "key": { "$q": "value } }
$gt	Matches values that are greater than a specified value.	{ "key": { "$gt": 4 } }
$gte	Matches values that are greater than or equal to a specified value.	{ "key": { "$gte": 4 } }
$lt	Matches values that are less than a specified value.	{ "key": { "$lt": 4 } }
$lte	Matches values that are less than or equal to a specified value.	{ "key": { "$lte": 4 } }
$in	Matches any of the values specified in an array.	{ "key": { "$in": [ 1, 2, 4 ] } }
$exists	Mathces values that either have (when set to true) or not have (when set to false) certain attributes	{ "key": { "$exists": true } }
$autocomplete	Mathces values that start with the specified string value	{ "key": { "$autocomplete": "value" } }
It’s also possible to combine filter expressions with the following operators:

Name	Description	Example
$and	Matches all the values specified in an array.	{ "$and": [ { "key": { "$in": [ 1, 2, 4 ] } }, { "some_other_key": 10 } ] }
$or	Matches at least one of the values specified in an array.	{ "$or": [ { "key": { "$in": [ 1, 2, 4 ] } }, { "key2": 10 } ] }
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Querying Calls
Next
Keeping The Call Alive In Background
© Getstream.io, Inc. All Rights Reserved.    , 
Video and Audio
/
Docs
/
React Native
/
Keeping The Call Alive In Background
Keeping The Call Alive In Background
One of the crucial functionalities of a video or audio calling application is to keep the call alive in the background. On this page, we focus on what must be added to your app to support this. After enabling, the user of your app will notice that the call is kept alive even if the app goes to the background as they will still hear the remote audio streams while the app is kept in the background.

Android Setup
In Android, we use a foreground service to keep the call alive. The SDK will automatically create and manage the foreground service. The first step is to install the Notifee library so that SDK can handle a foreground service. To install the Notifee library, run the following command in your terminal of choice:

Notifee version 9 or above is required to handle foreground service permissions for the calls.


Expo

React Native
Terminal
npx expo install @notifee/react-native
The next step is, in order to be able to use the foreground service, some declarations need to be added the AndroidManifest.xml. In app.json, in the plugins field, add true to the androidKeepCallAlive property in the @stream-io/video-react-native-sdk plugin. This will add the declarations automatically.

app.json
{
 "plugins": [
      [
        "@stream-io/video-react-native-sdk",
        {
           "androidKeepCallAlive": true
        }
      ],
      // your other plugins
  ]
}
If Expo EAS build is not used, please do npx expo prebuild --clean to edit the AndroidManifest.xml again after adding the config plugin property.

When uploading the app to the Play Store, it is essential to declare the permissions for foreground services in the Play Console and provide an explanation for their use. This includes adding a link to a video that demonstrates how the foreground service is utilized during video and audio calls. This procedure is required only once. For more details, click here. The added permissions are:

android.permission.FOREGROUND_SERVICE_CAMERA - To access camera when app goes to background
android.permission.FOREGROUND_SERVICE_MICROPHONE - To access microphone when app goes to background
android.permission.FOREGROUND_SERVICE_CONNECTED_DEVICE - To access bluetooth headsets when app goes to background
android.permission.FOREGROUND_SERVICE_DATA_SYNC - To keep video/audio calls alive when both camera and microphone access permissions were not granted
Request for notification permissions in Android
At an appropriate place in your app, request for notification permissions from the user on Android. Below is a small example of how to request permissions:


import { PermissionsAndroid } from "react-native";
PermissionsAndroid.request(PermissionsAndroid.PERMISSIONS.POST_NOTIFICATIONS);
Optional: override the default configuration of the foreground service notifications

import { StreamVideoRN } from "@stream-io/video-react-native-sdk";
import { AndroidImportance } from "@notifee/react-native";
StreamVideoRN.updateConfig({
  foregroundService: {
    android: {
      // see https://notifee.app/react-native/reference/nativeandroidchannel
      // for the various properties that can be used
      channel: {
        id: "stream_call_foreground_service",
        name: "Service to keep call alive",
        lights: false,
        vibration: false,
        importance: AndroidImportance.DEFAULT,
      },
      // you can edit the title and body of the notification here
      notificationTexts: {
        title: "Video call is in progress",
        body: "Tap to return to the call",
      },
      // you can optionally add a promise to run in the foreground service
      taskToRun: (call) =>
        new Promise(() => {
          console.log(
            "jumping to foreground service foreground service with call-cid",
            call.cid,
          );
        }),
    },
  },
});
iOS Setup
The way to keep audio alive in the background is to enable the audio background mode. When you enable this capability, your app’s audio playback will continue to play when users lock their iOS device or switch to another app. In Xcode: Open the Info.plist file and add audio in UIBackgroundModes. By editing this file with a text editor, you should see:


<key>UIBackgroundModes</key>
<array>
  <string>audio</string>
</array>
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Querying Call Members
Next
Manage Native Permissions
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Manage Native Permissions
Manage Native Permissions
In this guide, we will create a function to request the native permissions required for the app.

Once the function is called, we should see permissions being requested like below:

Preview of the final result
Setup
Ensure that relevant permissions are declared in your AndroidManifest.xml and Info.plist as mentioned in the installation(Native CLI or Expo) guide.

Additionally, to easily request permissions on both platforms, we will use the react-native-permissions library. You can run the following command to install it:

Terminal
yarn add react-native-permissions
Do not forget to perform the additional setup steps for iOS mentioned in the react-native-permissions library documentation

Step 1 - Add a function to request permissions in the app
In this step, we create a function called requestAndUpdatePermissions. This function will be responsible for requesting permissions.

src/utils/requestAndUpdatePermissions.ts
import { Platform } from "react-native";
import { PERMISSIONS, requestMultiple } from "react-native-permissions";
export const requestAndUpdatePermissions = async () => {
  if (Platform.OS === "ios") {
    // Request camera and mic permissions on iOS
    const results = await requestMultiple([
      PERMISSIONS.IOS.CAMERA,
      PERMISSIONS.IOS.MICROPHONE,
    ]);
  } else if (Platform.OS === "android") {
    // Request camera, mic, bluetooth and notification permissions on Android
    const results = await requestMultiple([
      PERMISSIONS.ANDROID.CAMERA,
      PERMISSIONS.ANDROID.RECORD_AUDIO,
      PERMISSIONS.ANDROID.BLUETOOTH_CONNECT,
      PERMISSIONS.ANDROID.POST_NOTIFICATIONS,
    ]);
  }
};
Step 2 - Use the function on your desired screen
In this final step, we use the requestAndUpdatePermissions function in the screen of our choice. As an example below, we use it in the screen where we pass the call object to the SDK.


import { useEffect } from "react";
import { requestAndUpdatePermissions } from "src/utils/requestAndUpdatePermissions";
import { StreamVideo, StreamCall } from "@stream-io/video-react-native-sdk";
const MyApp = () => {
  // request permissions on mount
  useEffect(() => {
    requestAndUpdatePermissions();
  }, []);
  return (
    <StreamVideo client={client}>
      <StreamCall call={call}>{/*  You UI */}</StreamCall>
    </StreamVideo>
  );
};
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Keeping The Call Alive In Background
Next
Permissions & Moderation
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Permissions & Moderation
Permissions & Moderation
In many types of calls, there is a requirement for providing different users with certain permissions and capabilities. A typical example is a webinar where the host wants to control who can speak or who can share their video or screen.

The Stream Video SDK provides a certain set of permissions and capabilities that can be used to control the behavior of participants in a call.

Conceptual overview
Roles
The Stream Video API allows assigning roles to users. Each user has a global role, and they will also have a call-level role for each call they join. The Stream Video API provides a set of predefined roles, but it’s also possible to create your own roles.

Call types
Call types also allow for a more granular control system:

you can enable/disable certain features on a call type level
you can configure how each call-level role works on a call type level
Capabilities
Based on a user’s roles and the call type settings, we can determine which actions are allowed for a user joined to a specific call. The capabilities of the local user live in the state of the call instance.

In our React Native Video SDK, you can use the useOwnCapabilities hook.


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useOwnCapabilities } = useCallStateHooks();
const ownCapabilities = useOwnCapabilities();
Permissions
As soon as you join a call, the Call instance would allow you to check the permissions of the local user or perform some permission-related actions:

Check permissions

import { OwnCapability } from "@stream-io/video-react-native-sdk";
const call = streamVideoClient.call(type, id);
const canSendAudio = call.permissionsContext.hasPermission(
  OwnCapability.SEND_AUDIO,
);
In our React Native Video SDK, you can use the useHasPermissions hook to check for permissions.


import {
  useCallStateHooks,
  OwnCapability,
} from "@stream-io/video-react-native-sdk";
const { useHasPermissions } = useCallStateHooks();
const canSendAudio = useHasPermissions(OwnCapability.SEND_AUDIO);
Request permissions
Every user may request permission to perform certain actions depending on the call type and call settings. For example, in an audio-room call type, only the hosts have send-audio permission by default. Other users should request this permission before they can start sending audio if the call settings allow it.


import { OwnCapability } from "@stream-io/video-react-native-sdk";
const call = streamVideoClient.call(type, id);
if (!call.permissionsContext.canRequest(OwnCapability.SEND_AUDIO)) {
  console.log("The host has disabled the ability to request this permission");
  return;
}
await call.requestPermissions({
  permissions: [OwnCapability.SEND_AUDIO],
});
Approving permission requests
Call hosts and moderators can approve permission requests from other users. Whenever a user requests a certain permission, a call.permission_request event will be emitted on the Call instance. You can listen to this event and approve the request.


import {
  PermissionRequestEvent,
  StreamCallEvent,
} from "@stream-io/video-react-native-sdk";
const call = streamVideoClient.call(type, id);
call.on("call.permission_request", async (event: StreamCallEvent) => {
  const request = event as PermissionRequestEvent;
  if (shouldApproveRequest(request)) {
    await call.grantPermissions(request.user.id, request.permissions);
  }
});
Moderation
At any time, a moderator or host can decide to either grant or revoke certain permission to any participant.


import { OwnCapability } from "@stream-io/video-react-native-sdk";
const call = streamVideoClient.call(type, id);
await call.updateUserPermissions({
  user_id: "demo-user",
  grant_permission: [OwnCapability.SEND_AUDIO, OwnCapability.SEND_VIDEO],
  revoke_permissions: [OwnCapability.SCREENSHARE],
});
// alternate API for granting user permissions:
await call.grantPermissions("demo-user", [
  OwnCapability.SEND_AUDIO,
  OwnCapability.SEND_VIDEO,
]);
// alternate API for revoking user permissions:
await call.revokePermissions("demo-user", [OwnCapability.SCREENSHARE]);
The end user would get notified via a WebSocket event with a type: call.permissions_updated. In the case of revoked permissions, the SDK would automatically stop publishing the appropriate tracks.

Ending call for everyone
In some cases, a moderator or host may want to end the call for everyone.


const call = streamVideoClient.call(type, id);
await call.endCall();
This operation will emit call.ended event to every participant in the call. The SDK would automatically stop publishing any tracks and leave the call.

Ended calls can’t be re-joined.

Users
Blocking and Unblocking
In some cases, a moderator or host may want to block a participant from joining the call.


const call = client.call(type, id);
await call.blockUser("user-id");
// to unblock
await call.unblockUser("user-id");
Kicking
Kicking a participant is a softer version of blocking. The participant will be disconnected from the call, but they will be able to re-join the call.


const call = client.call(type, id);
await call.kickUser({ user_id: "user-id" });
// you can use this shortcut to block the user from re-joining the call:
await call.kickUser({ user_id: "user-id", block: true });
Muting
In addition to granting or revoking permissions, a moderator or host can also mute a participant. This is a common scenario as quite often, participants may be a source of unwanted noise or distraction.


const call = client.call(type, id);
await call.muteUser("demo-user-id", "audio");
await call.muteUser("demo-user-id", "video");
await call.muteUser("demo-user-id", "screenshare");
// or, mute in bulk
await call.muteUser(["demo-user-id", "demo-user-id-2"], "audio");
// or, muting self
await call.muteSelf("audio");
// or, muting others
await call.muteOthers("audio");
// or, mute all, including self.
await call.muteAllUsers("audio");
This operation doesn’t revoke any permission, and the user would still be able to un-mute itself.

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Manage Native Permissions
Next
Reactions
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Reactions
Reactions
Reactions allow call participants to send emojis in real-time.

Custom events let participants send and receive arbitrary WebSocket messages. For example, if you want to implement a drawing feature in your call, you can use custom events for synchronizing the drawing board between participants.

Reactions
CallControls optionally utilizes ReactionsButton component that support reactions out-of-the-box, but for advanced use-cases you can also build your own reaction system.

Sending reactions
You can send a reaction using the sendReaction method of a Call instance.


const call: Call;
await call.sendReaction({ type: "raised-hand" });
The value of the type attribute can be any string.

It’s also possible to provide additional data for the reaction:


const call: Call;
await call.sendReaction({
  type: "raised-hand",
  emoji_code: ":raise-hand:",
  custom: { clearAfterTimeout: true },
});
The emoji_code attribute is used by the SDK components to decide which emoji to display on the UI.

The custom property can contain any data.

Receiving reactions
Reactions are only delivered to clients that are watching the call.

The participant state will contain the latest reaction of each participant:


const { useParticipants } = useCallStateHooks();
const participants = useParticipants();
const reactions = participants.map((p) => p.reaction);
You can also subscribe to the call.reaction_new WebSocket event to receive reactions. For more information, check out our Events guide.

Clearing reactions
If you’re using the participant state for receiving reactions, you can also clear the latest reaction using the resetReaction method:


const call: Call;
const { useParticipants } = useCallStateHooks();
const participants = useParticipants();
call.resetReaction(participants[0].sessionId);
This is a local action, it won’t send any WebSocket messages. It’s helpful if you only want to display reactions for a set period of time.

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Permissions & Moderation
Next
Custom Events
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Custom Events
Custom Events
You can use custom events to send data among the participants in the call. This is a realtime layer that you can use to broadcast your own events to.

Sending custom events
For example, if you are building a collaborative drawing app, you can send the coordinates to the other participants with the following code:


await call.sendCustomEvent({
  type: "draw",
  x: 10,
  y: 30,
});
Please note that the total payload for these events is limited to 5KB in size.

Receiving custom events
Custom events are only delivered to clients that are watching the call.

To receive custom events, you need to subscribe to the custom event on the call instance:


const unsubscribe = call.on("custom", (event: CustomVideoEvent) => {
  const payload = event.custom;
  if (payload.type === "draw") {
    console.log(`Received draw event: x=${payload.x}, y=${payload.y}`);
  }
});
// Unsubscribe when you no longer need to listen to custom events
unsubscribe();
For more information, check out our Events guide.

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Reactions
Next
Participant sorting
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Participant sorting
Participant sorting
The Participant Sorting API is a powerful tool built on top of the internal Comparator<T> API, providing developers with the ability to sort participants in various scenarios. This API offers common comparators and built-in presets that can be easily customized or used out-of-the-box, making participant sorting a seamless experience.

When dealing with real-time communication applications, it is often necessary to sort participants based on specific criteria. Whether you need to adjust the sorting in existing view layouts or define new sorting presets, the Participant Sorting API is here to simplify the process.

By using the Comparator<T> API and the provided built-in comparators and presets, developers can effortlessly sort participants according to their requirements.

Comparator<T> API overview
The Comparator<T> API is the foundation upon which the Participant Sorting API is built. It defines a function type Comparator<T> that takes two arguments a and b of type T and returns -1, 0, or 1 based on the comparison between the two items. This API allows developers to create custom comparators tailored to their specific needs.

Ultimately, this API can be used in conjunction with the Array.sort method to sort any type of data.


import {
  Comparator,
  combineComparators,
  conditional,
  descending,
} from "@stream-io/video-react-native-sdk";
type Participant = {
  id: number;
  name: string;
};
// comparator that sorts by name in ascending order
const byName: Comparator<Participant> = (a, b) => {
  if (a.name < b.name) return -1;
  if (a.name > b.name) return 1;
  return 0;
};
// comparator that sorts by id in ascending order
const byId: Comparator<Participant> = (a, b) => {
  if (a.id < b.id) return -1;
  if (a.id > b.id) return 1;
  return 0;
};
// comparator that sorts by age in ascending order
const byAge: Comparator<Participant> = (a, b) => {
  if (a.age < b.age) return -1;
  if (a.age > b.age) return 1;
  return 0;
};
// creates a new comparator that sorts by name in descending order
const byNameDescending: Comparator<Participant> = descending(byName);
// `conditional` creates a new comparator that applies the provided comparator only
// if the provided predicate returns `true`. The `predicate` itself, takes the two arguments
// and returns a boolean value.
const byAgeIfEnabled: Comparator<Participant> = conditional(
  (a, b) => opts.isSortByAgeEnabled,
)(descending(byAge));
// combineComparator creates a new Comparator<T> that combines the provided comparators in one.
// this comparator will sort by name in descending order, by age if enabled,
// and then by id in ascending order
const sortingCriteria = combineComparators(
  byNameDescending,
  byAgeIfEnabled,
  byId,
);
// participants array
const sorted = [p1, p2, p3].sort(sortingCriteria);
The Comparator<T> API is quite generic and can be used to sort any type of data. Works great in a pair with the Array.sort API.

Built-in common comparators
The Participant Sorting API provides a set of common comparators that cover common sorting scenarios. These comparators are specifically designed for participant sorting and offer convenience when defining sorting criteria.

The built-in common comparators include:

dominantSpeaker: Sorts participants based on their dominance in the call.
speaking: Sorts participants based on whether they are currently speaking.
screenSharing: Sorts participants based on whether they are currently screen sharing.
publishingVideo: Sorts participants based on whether they are currently publishing video.
publishingAudio: Sorts participants based on whether they are currently publishing audio.
pinned: Sorts participants based on whether they are pinned in the user interface.
reactionType(type): Sorts participants based on the type of reaction they have.
role(...roles): Sorts participants based on their assigned role.
name: Sorts participants based on their names.
withParticipantSource: Sorts participants based on their participant source.
All of these comparators are available in the @stream-io/video-react-native-sdk package and can be imported as follows:


import {
  dominantSpeaker,
  speaking,
  screenSharing,
  publishingVideo,
  publishingAudio,
  pinned,
  reactionType,
  role,
  name,
  withParticipantSource,
} from "@stream-io/video-react-native-sdk";
// ...
These built-in comparators serve as a starting point for sorting participants and can be used individually or combined to create more complex sorting criteria.

Sorting customization on the call level
The Participant Sorting API allows dynamic sorting customization during runtime. Developers can use the call.setSortParticipantsBy(comparator) API to change the sorting criteria based on user interactions or application logic. This flexibility empowers developers to provide sorting controls within their application, giving users the ability to customize participant sorting according to their preferences.

Let’s take a look at an example:


import {
  useCall,
  combineComparators,
  dominantSpeaker,
  publishingVideo,
  publishingAudio,
  screensharing,
  speaking,
  reactionType,
  pinnned,
  SfuModels,
} from "@stream-io/video-react-native-sdk";
// ... boilerplate code
// we take the existing call instance
const call = useCall();
// we create a new comparator that combines the built-in comparators
// and sorts participants by the following criteria:
const comparator = combineComparators(
  pinned, // 1. pinned participants first
  screenSharing, // 2. participants who are screensharing
  dominantSpeaker, // 3. dominant speaker
  reactionType("raised-hand"), // 4. participants with raised hand
  speaking, // 5. participants currently speaking
  withParticipantSource(
    SfuModels.ParticipantSource.RTMP,
    SfuModels.ParticipantSource.SRT,
  ), // 6. participants with video ingress source (e.g.: OBS)
  publishingVideo, // 7. participants publishing video
  publishingAudio, // 8. participants publishing audio
  // 9. everyone else
);
// will apply the new sorting criteria immediately
call.setSortParticipantsBy(comparator);
In some scenarios, we might want to have special sorting criteria for a specific component in our app. For example, in the participant list component, we might want to sort participants by name.

For this purpose, we have extended the built-in useParticipants hook with a sortBy: Comparator<StreamVideoParticipant> option parameter.


import {
  combineComparators,
  name,
  useCallStateHooks,
} from "@stream-io/video-react-native-sdk";
const { useParticipants } = useCallStateHooks();
// this will override the call's default sorting criteria
// and will return a list of participants sorted by name
const participants = useParticipants({ sortBy: name });
// you can also provide your custom comparator
const myComparator = combineComparators(/* ... */);
const participants = useParticipants({ sortBy: myComparator });
When using custom comparator in combination with the useParticipants hook, please make sure to provide a stable reference to the comparator. Otherwise, you might end up with unexpected behavior (unexpected re-renders, etc.).

Our proposal is to use stateless comparators defined outside the component’s scope. In case you need to use a stateful comparator, please make sure to memoize it using React.useMemo or React.useCallback hooks.


// stateless comparator
const myStatelessComparator = combineComparators(/* ... */);
export const MyComponent = () => {
  const { useParticipants } = useCallStateHooks();
  // component scope
  const participants1 = useParticipants({ sortBy: myStatelessComparator });
  // memoized comparator
  const myStatefulComparator = React.useMemo(
    () => combineComparators(/* ... */),
    [dependency1, dependency2],
  );
  const participants2 = useParticipants({ sortBy: myStatefulComparator });
  // ...
};
Built-in sorting presets
To further simplify participant sorting, the Participant Sorting API offers built-in presets. These presets are pre-configured sorting criteria linked to specific call types, reducing the effort required to define sorting rules.

The following presets are available:

defaultSortPreset: The default sorting preset applicable to general call scenarios.
speakerLayoutSortPreset: A preset specifically designed for the 'default' call type, optimizing participant sorting for speaker layout view.
livestreamOrAudioRoomSortPreset: A preset tailored for the 'livestream' and 'audio_room' call types, ensuring optimal participant sorting in livestream or audio room scenarios.
These presets are directly applied to the call’s sorting mechanism. For custom call types, unless specified otherwise, our SDK would use the defaultSortPreset preset.

All of these presets are available in the @stream-io/video-react-native-sdk package and can be imported as follows:


import {
  defaultSortPreset,
  speakerLayoutSortPreset,
  livestreamOrAudioRoomSortPreset,
} from "@stream-io/video-react-native-sdk";
For your custom call types, you can define your participant sorting presets and register them generally in the SDK.

Check the next section to learn how.

Sorting customization on the call type
Sometimes, you want to keep your UI components free from sorting logic and instead, define sorting criteria per call type. To do so, you can register your sorting presets for your custom call types or override the existing ones by using our SDKs CallTypes registry.


import { combineComparators, CallTypes, CallType } from '@stream-io/video-react-native-sdk';
// setup your custom sorting preset
const myCustomSortPreset = combineComparators(/* ... */);
// update existing type
CallTypes.get('default').options.sortParticipantsBy = myCustomSortPreset;
// register new type
CallTypes.register(new CallType('my-custom-type', {
  options: {
    sortParticipantsBy: myCustomSortPreset,
  },
});
Disabling participant sorting
In some cases, you may want to disable participant sorting altogether. This can be achieved by setting our special noopComparator as the sorting criteria of the Call or the CallType.


import { noopComparator, useCall } from "@stream-io/video-react-native-sdk";
const call = useCall();
call.setSortParticipantsBy(noopComparator());
Alternatively, you may want to opt out of sorting for specific parts of your application where participant order is irrelevant or excessive rendering should be avoided. In that case, you can use the useRawParticipants call state hook:


import { useCallStateHooks } from "@stream-io/video-react-native-sdk";
const { useRawParticipants } = useCallStateHooks();
const unsortedParticipants = useRawParticipants();
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Custom Events
Next
User ratings
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
User ratings
User ratings
Introduction
Asking your users to rate their experience at the end of a call is a best practice that allows you to capture important feedback and helps you improve your product. It is highly recommended to use the feedback API to collect this information.

The ratings are also rendered inside the dashboard stats screen, allowing you to see the average rating of your calls and the feedback provided by your users.

User ratings are also used by Stream to improve the quality of our services. We use this feedback to identify issues and improve the overall quality of our video calls.

Submit Feedback API
Our React Native Video SDK provides an API for collecting this feedback which later can be seen in the call stats section of our dashboard.


await call.submitFeedback(
  rating, // a rating from 1 to 5,
  {
    reason: "I could not select my external camera from the UI", // some text feedback (optional)
    custom: {
      role: "patient",
    },
  },
);
Example

import React, { useState } from 'react';
import {
  View,
  Text,
  TouchableOpacity,
  StyleSheet,
  Modal,
  Image,
} from 'react-native';
import { useCall } from '@stream-io/video-react-native-sdk';
import Star from '../assets/Star';
import Close from '../assets/Close';
const FeedbackModal: = () => {
  const call = useCall();
  const [selectedRating, setSelectedRating] = useState<number | null>(null);
  const handleRatingPress = (rating: number) => {
    setSelectedRating(rating);
    await call
      ?.submitFeedback(Math.min(Math.max(1, rating), 5), {
        reason: '<no-message-provided>',
      })
      .catch((err) => console.warn('Failed to submit call feedback', err));
  };
  return (
    <Modal
      transparent
      visible={visible}
      onRequestClose={onClose}
    >
      <TouchableOpacity style={styles.overlay} onPress={onClose}>
        <View style={[styles.modal]}>
          <View style={styles.top}>
            <View style={styles.topRight}>
              <TouchableOpacity onPress={onClose} style={[styles.closeButton]}>
                <IconWrapper>
                  <Close
                    color={colors.typeSecondary}
                    size={variants.roundButtonSizes.sm}
                  />
                </IconWrapper>
              </TouchableOpacity>
            </View>
          </View>
          <Image source={require('../assets/feedbackLogo.png')} />
          <View style={styles.textContainer}>
            <Text style={styles.title}>We Value Your Feedback!</Text>
            <Text style={styles.subtitle}>
              Tell us about your video call experience.
            </Text>
          </View>
          <View style={styles.ratingContainer}>
            {[1, 2, 3, 4, 5].map((rating) => (
              <TouchableOpacity
                key={rating}
                onPress={() => handleRatingPress(rating)}
                style={[styles.ratingButton]}
              >
                <Star
                  color={
                    selectedRating && selectedRating >= rating
                      ? colors.iconAlertSuccess
                      : colors.typeSecondary
                  }
                />
              </TouchableOpacity>
            ))}
          </View>
          <View style={styles.bottom}>
            <View style={styles.left}>
              <Text style={styles.text}>Very Bad</Text>
            </View>
            <View style={styles.right}>
              <Text style={styles.text}>Very Good</Text>
            </View>
          </View>
        </View>
      </TouchableOpacity>
    </Modal>
  );
};
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Participant sorting
Next
Livestreaming
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Overview
Overview
Stream SDK aims to make it as easy as possible to build your own video calling, audio rooms, and live streams. We support a low-level client, guides on building your own UI, and several pre-built UI components. If you quickly want to add calling to your app, you can do that just in an hour with these UI components.

Rendering Participant
If you want to render a participant’s video together with:

A label/name for the participant
Network quality indicator
Mute/unmute indicator
Fallback for when video is muted
Reactions
We can use ParticipantView:


<ParticipantView participant={participant} />
You will see the result as below:

Participant Camera On
Participant Camera Off
Video Call UI
You can use the CallContent:

Call Participants Layout: A call video that renders the full participants of the call.
Controls: Content is shown that allows users to trigger different actions to control a joined call.

const App = () => {
  return (
    <View style={styles.container}>
      <CallContent />
    </View>
  );
};
const styles = StyleSheet.create({
  container: {
    flex: 1,
  },
});
Preview of Video Call UI
Ringing (Incoming/Outgoing calls)
You can implement incoming/outgoing screens using our RingingCallContent component:

It displays the IncomingCall/OutgoingCall components depending upon the call states.
After the call is accepted its displays the CallContent component.
While the call is in joining state it shows JoiningCallIndicator component.
Incoming Call
Outgoing Call

const App = () => {
  return (
    <View style={styles.container}>
      <RingingCallContent />
    </View>
  );
};
const styles = StyleSheet.create({
  container: {
    flex: 1,
  },
});
UI Component Customization
Stream SDK provides highly customizable UI components. Therefore, you can adjust each style or implement your own UI for each part of the components. This list describes what you can do with Stream SDK’s UI components:

You can also build your UI components from scratch with our low-level UI component using our UI Cookbook.
Use our library of built-in components.
Mix & Match between your own and built-in components.
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Livestreaming
Next
Theme
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Reject call when busy
Reject call when busy
In this documentation article, we will explore how to setup automatic call rejection on callee side when callee is in another ringing call. Also we will show how this information can be presented on the caller side.

Reject call when busy
To enable this we need to configure the client with the option rejectCallWhenBusy set to true.


import { StreamVideoClient } from "@stream-io/video-react-native-sdk";
const client = StreamVideoClient.getOrCreateInstance({
  apiKey,
  tokenProvider,
  user,
  options: { rejectCallWhenBusy: true },
});
If rejectCallWhenBusy is set to true the call will be automatically rejected by the callee. The call rejected event will be attached with reason busy. In this case, on the caller side, the SDK will automatically play a busy tone sound. Additionally, you can opt to show a visual indication on the caller side that the callee is busy in another call. The snippet below is an example of how we can show a alert dialog with a message from the call.rejected event handler on our videoClient:


import { useStreamVideoClient } from "@stream-io/video-react-native-sdk";
import { Alert } from "react-native";
const videoClient = useStreamVideoClient();
useEffect(() => {
  if (!videoClient) return;
  return videoClient?.on("call.rejected", async (event) => {
    const isCallCreatedByMe =
      event.call.created_by.id === _videoClient?.state.connectedUser?.id;
    const calleeName = event.user.name ?? event.user.id;
    const isCalleeBusy = isCallCreatedByMe && event.reason === "busy";
    if (isCalleeBusy) {
      Alert.alert("Call rejected", `User: ${calleeName} is busy.`);
    }
  });
}, [videoClient]);
Reject call when busy
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Low Bandwidth
Next
Overview
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Low Bandwidth
Low Bandwidth
Our servers can detect if you are on a low-bandwidth connection and will automatically adjust the video quality to ensure smooth playback. However, sometimes even reduced quality may not be enough for a good experience, and our system may decide to opt the local user out from consuming some or all remote videos. The video pause feature improves call quality by automatically turning off incoming video streams, resulting in an audio-only mode in response to deteriorating network conditions on the subscriber side.

In such cases, you will see an icon in the participant’s label indicating that the video is being paused due to bandwidth constraints.

Low bandwidth
Low Bandwidth Optimization toggling
The Low Bandwidth optimization is enabled by default on an SDK level. However, integrators can decide to opt out of this feature accordingly to their use case:


import { SfuModels } from "@stream-io/video-react-native-sd";
const call = client.call(type, id);
await call.disableClientCapabilities(
  SfuModels.ClientCapability.SUBSCRIBER_VIDEO_PAUSE,
);
// use call.enableClientCapabilities(...) to re-enable the feature
await call.join();
This signals to the backend that the client supports dynamic video pausing, allowing the system to optimize media delivery under limited network conditions.

Observing Paused Tracks
The information about server-side paused tracks lives on the participant.pausedTracks property. You can observe changes to this property to customize the UI further, such as showing a message or changing the participant’s label.


import {
  useCallStateHooks,
  SfuModels,
} from "@stream-io/video-react-native-sdk";
export const MyComponent = () => {
  const { useParticipants } = useCallStateHooks();
  const participants = useParticipants();
  const participantsWithPausedVideoTracks = participants.filter((p) =>
    p.pausedTracks?.includes(SfuModels.TrackType.VIDEO),
  );
  return (
    <>
      {participantsWithPausedVideoTracks.length > 0 && (
        <ShowPausedIncomingVideoNotification />
      )}
      <MyCalLayoutUI />
    </>
  );
};
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Audio Volume Indicator
Next
Reject call when busy
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Audio Volume Indicator
Audio Volume Indicator
In this documentation article, we will explore how to build sound detection by using our useSpeechDetection() hook.

AudioVolumeIndicator component
With invoking our useSpeechDetection() hook, we get info about the audio level and whether sound is detected from the microphone. The audioLevel can have values from 0 to 1.

The SpeechIndicator component will show sound detection by dynamically expanding and contracting three vertical lines when we pass isSpeaking=true or three static dots when isSpeaking=false.

Image of audio volume indicator
This can be useful in Lobby components where the call is not started yet.


import {
  useSpeechDetection,
  SpeechIndicator,
} from "@stream-io/video-react-native-sdk";
export const AudioVolumeIndicator = () => {
  const { audioLevel, isSoundDetected } = useSpeechDetection();
  const isSpeaking = isSoundDetected && audioLevel > 0.1;
  return <SpeechIndicator isSpeaking={isSpeaking} />;
};
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Network Disruptions
Next
Low Bandwidth
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Network Disruptions
Network Disruptions
Connection problems can occur during a call, for example when switching networks or if the signal is poor. In this case the SDK will try to reconnect automatically.

The call.setDisconnectionTimeout method allows you to specify how long a user can remain disconnected before being removed from the call. This is particularly useful when users experience short network interruptions and are able to reconnect quickly. By setting a timeout you ensure that users are only dropped if their disconnection lasts longer than the specified time.

Setting the disconnection timeout
Once the call has been created, you can set a disconnection timeout that defines how long a user can stay disconnected before being dropped. Here’s how to do it:


call.setDisconnectionTimeout(30); // Try to reconnect for 30 seconds
call.setDisconnectionTimeout(0); // try to reconnect indefinitely (default)
By default the disconnection timeout is set to 0, allowing the user to remain in the call until their connection restores or the call is ended.

Notify user after disconnection
Current reconnection state is reflected in the calling state.

After a specified timeout, if the connection is not reestablished, the calling state becomes CallingState.RECONNECTING_FAILED. We can subscribe to it to display a special UI, notifying user after disconnection:


const { useCallCallingState } = useCallStateHooks();
const callingState = useCallCallingState();
if (callingState === CallingState.RECONNECTING_FAILED) {
  return <ConnectionError />; // display connection error UI
}
Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
Safe Area Insets
Next
Audio Volume Indicator
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Expo
Expo
If you are on version 1.2 or below, you would need to upgrade to 1.3 or above to follow the setup. As 1.3 release had breaking changes with respect to setting up of push notifications. We recommend to update to the current latest version.

This guide discusses how to add push notifications for ringing calls to your project. It will discuss both Android and iOS and go through all the necessary steps.

The normal user experience in a ringing app, when a user receives a call, is to show a push notification. The user can then interact with the notification to accept or reject the call. In this guide, you will learn how to set up your Expo app to get push notifications from Stream for the incoming calls that your user will receive.

Android preview	iOS preview
Android preview of the Firebase push notification
iOS preview of VoIP notification using Apple Push Notification service (APNs)
Full-screen notifications are displayed when the phone screen is locked or the app is active (foreground state). However, when the app is terminated or in the background and the screen is awake, notifications may appear as heads-up notifications instead of a full-screen alerts.

Add push provider credentials to Stream
Please follow the below guides for adding appropriate push providers to Stream:

Android - Firebase Cloud Messaging
iOS - Apple Push Notification Service (APNs)
Install Dependencies
Terminal
npx expo install \
  @react-native-firebase/app \
  @react-native-firebase/messaging \
  @notifee/react-native \
  react-native-voip-push-notification \
  react-native-callkeep \
  @config-plugins/react-native-callkeep
So what did we install precisely?

@react-native-firebase/app and @react-native-firebase/messaging for handling incoming Firebase Cloud Messaging notifications on Android.
@notifee/react-native - is used to customize and display push notifications.
react-native-voip-push-notification for handling incoming PushKit notifications on iOS.
react-native-callkeep and @config-plugins/react-native-callkeep for reporting incoming calls to iOS CallKit.
Add Firebase credentials
To create a Firebase project, go to the Firebase console and click on Add project.
In the console, click the setting icon next to Project overview and open Project settings. Then, under Your apps, click the Android icon to open Add Firebase to your Android app and follow the steps. Make sure that the Android package name you enter is the same as the value of android.package from your app.json.
After registering the app, download the google-services.json file and place it in your project’s root directory.
In app.json, add an android.googleServicesFile field with the relative path to the downloaded google-services.json file. If you placed it in the root directory, the path is:
app.json
{
  "android": {
    "googleServicesFile": "./google-services.json"
  }
}
Similarly, for iOS, in the console, click the setting icon next to Project overview and open Project settings. Then, under Your apps, click the iOS icon to open Add Firebase to your Apple app and follow the steps. Make sure that the Apple bundle ID you enter is the same as the value of ios.bundleIdentifier from your app.json.
After registering the app, download the GoogleService-Info.plist file and place it in your project’s root directory.
In app.json, add an ios.googleServicesFile field with the relative path to the downloaded GoogleService-Info.plist file. If you placed it in the root directory, the path is:
app.json
{
  "ios": {
    "googleServicesFile": "./GoogleService-Info.plist"
  }
}
The google-services.json and GoogleService-Info.plist files contain unique and non-secret identifiers of your Firebase project. For more information, see Understand Firebase Projects.

We will not be using firebase for iOS. But it is necessary for the setup for react-native-firebase to have the GoogleService-Info.plist file.

iOS - Notifications entitlement
On Expo SDK 51 above, Notifications entitlement is no longer always added to iOS projects. Therefore, the following entitlements property needs to be added to app.json:

app.json
{
  "expo": {
    "ios": {
      "entitlements": {
        "aps-environment": "production"
      }
    }
  }
}
Add the config plugin properties
In app.json, in the plugins field, add the ringingPushNotifications property to the @stream-io/video-react-native-sdk plugin. Also, add the @config-plugins/react-native-callkeep plugin.

app.json
{
  "plugins": [
    [
      "@stream-io/video-react-native-sdk",
      {
        "ringingPushNotifications": {
          "disableVideoIos": false,
          "includesCallsInRecentsIos": false,
          "showWhenLockedAndroid": true
        },
        "androidKeepCallAlive": true
      }
    ],
    "@config-plugins/react-native-callkeep",
    [
      "@config-plugins/react-native-webrtc",
      {
        "cameraPermission": "$(PRODUCT_NAME) requires camera access in order to capture and transmit video",
        "microphonePermission": "$(PRODUCT_NAME) requires microphone access in order to capture and transmit audio"
      }
    ],
    "@react-native-firebase/app",
    "@react-native-firebase/messaging",
    [
      "expo-build-properties",
      {
        "ios": {
          "useFrameworks": "static",
          "forceStaticLinking": [
            "RNFBApp",
            "RNFBMessaging",
            "stream-react-native-webrtc"
          ]
        }
      }
    ]
    // your other plugins
  ]
}
The disableVideoIos field is used for apps with audio only calls. Pass true to this property to disable video in iOS CallKit.
The includesCallsInRecentsIos field is used to show call history. Pass true to show the history of calls made in the iOS native dialer
The showWhenLockedAndroid field is used to display a full-screen notification for the incoming call when the phone is locked. Pass true to enable it.
For iOS only,
firebase-ios-sdk requires static frameworks then you want to configure expo-build-properties by adding "useFrameworks": "static".
Since Expo 54, forceStaticLinking is required for certain libraries when "useFrameworks": "static" is used.
The plugin adds a foreground service and the necessary permissions for Android. It shows incoming call notifications and keeps video/audio calls active when the app is in the background.

When uploading the app to the Play Store, declare these permissions in the Play Console and explain their usage, including a link to a video demonstrating the service. This is a one-time requirement. For more information, click here.

The added permissions are:

android.permission.FOREGROUND_SERVICE_CAMERA - To access camera when app goes to background
android.permission.FOREGROUND_SERVICE_MICROPHONE - To access microphone when app goes to background
android.permission.FOREGROUND_SERVICE_CONNECTED_DEVICE - To access bluetooth headsets when app goes to background
android.permission.FOREGROUND_SERVICE_DATA_SYNC - To keep video/audio calls alive when both camera and microphone access permissions were not granted
If Expo EAS build is not used, please do npx expo prebuild --clean to generate the native directories again after adding the config plugins.

Optional: Disable Firebase integration on iOS
React Native Firebase Messaging automatically registers the device with APNs to receive remote messages. But since we do not use Firebase on iOS, we can disable it via the firebase.json file that we can newly create:

{projectRoot}/firebase.json
{
  "react-native": {
    "messaging_ios_auto_register_for_remote_messages": false
  }
}
Add Firebase message handlers
To process the incoming push notifications, the SDK provides the utility functions that you must add to your existing or new Firebase notification listeners. Below is the snippet of how to add the firebase listeners:

src/utils/setFirebaseListeners.ts
import messaging from "@react-native-firebase/messaging";
import notifee from "@notifee/react-native";
import {
  isFirebaseStreamVideoMessage,
  firebaseDataHandler,
  onAndroidNotifeeEvent,
  isNotifeeStreamVideoEvent,
} from "@stream-io/video-react-native-sdk";
export const setFirebaseListeners = () => {
  // Set up the background message handler
  messaging().setBackgroundMessageHandler(async (msg) => {
    if (isFirebaseStreamVideoMessage(msg)) {
      await firebaseDataHandler(msg.data);
    } else {
      // your other background notifications (if any)
    }
  });
  // on press handlers of background notifications
  notifee.onBackgroundEvent(async (event) => {
    if (isNotifeeStreamVideoEvent(event)) {
      await onAndroidNotifeeEvent({ event, isBackground: true });
    } else {
      // your other background notifications (if any)
    }
  });
  // Optionally: set up the foreground message handler
  messaging().onMessage((msg) => {
    if (isFirebaseStreamVideoMessage(msg)) {
      firebaseDataHandler(msg.data);
    } else {
      // your other foreground notifications (if any)
    }
  });
  //  Optionally: on press handlers of foreground notifications
  notifee.onForegroundEvent((event) => {
    if (isNotifeeStreamVideoEvent(event)) {
      onAndroidNotifeeEvent({ event, isBackground: false });
    } else {
      // your other foreground notifications (if any)
    }
  });
};
Firebase message handlers:

The onMessage handler should not be added if you do not want notifications to show up when the app is in the foreground. When the app is in foreground, you would automatically see the incoming call screen.
The isFirebaseStreamVideoMessage method is used to check if this push message is a video related message. And only this needs to be processed by the SDK.
The firebaseDataHandler method is the callback to be invoked to process the message. This callback reads the message and uses the @notifee/react-native library to display push notifications.
Notifee event handlers:

The onForegroundEvent handler should not be added if you did not add foreground notifications above.
The isNotifeeStreamVideoEvent method is used to check if the event was a video related notifee event. And only this needs to be processed by the SDK.
The onAndroidNotifeeEvent method is the callback to be invoked to process the event. This callback reads the event and makes sure that the call is accepted or declined.
If you have disabled the initialization of Firebase on iOS, add the above method only for Android using the Platform-specific extensions for React Native.

For example, say you add the following files in your project:


setFirebaseListeners.android.ts
setFirebaseListeners.ts
The method above must only be added to the file that .android extension. The other file must add the method but do nothing like below:

setFirebaseListeners.ts
export const setFirebaseListeners = () => {
  // do nothing
};
Setup the push notifications configuration for the SDK
The SDK automatically processes the incoming push notifications once the setup above is done if the push notifications configuration has been set using StreamVideoRN.setPushConfig. Through this method, you can override the default notification texts and set the push provider name for both iOS and Android and provide your custom ringtone.

Below is an example of how this method can be called:

src/utils/setPushConfig.ts
import {
  StreamVideoClient,
  StreamVideoRN,
  User,
} from "@stream-io/video-react-native-sdk";
import { AndroidImportance } from "@notifee/react-native";
import AsyncStorage from "@react-native-async-storage/async-storage";
import { STREAM_API_KEY } from "../../constants";
export function setPushConfig() {
  StreamVideoRN.setPushConfig({
    // pass true to inform the SDK that this is an expo app
    isExpo: true,
    ios: {
      // add your push_provider_name for iOS that you have setup in Stream dashboard
      pushProviderName: __DEV__ ? "apn-video-staging" : "apn-video-production",
    },
    android: {
      // the name of android notification icon (Optional, defaults to 'ic_launcher')
      smallIcon: "ic_notification",
      // add your push_provider_name for Android that you have setup in Stream dashboard
      pushProviderName: __DEV__
        ? "firebase-video-staging"
        : "firebase-video-production",
      // configure the notification channel to be used for incoming calls for Android.
      incomingCallChannel: {
        id: "stream_incoming_call",
        name: "Incoming call notifications",
        // This is the advised importance of receiving incoming call notifications.
        // This will ensure that the notification will appear on-top-of applications.
        importance: AndroidImportance.HIGH,
        // optional: if you dont pass a sound, default ringtone will be used
        sound: "<url to the ringtone>",
      },
      // configure the functions to create the texts shown in the notification
      // for incoming calls in Android.
      incomingCallNotificationTextGetters: {
        getTitle: (userName: string) => `Incoming call from ${userName}`,
        getBody: (_userName: string) => "Tap to answer the call",
        getAcceptButtonTitle: () => "Accept",
        getDeclineButtonTitle: () => "Decline",
      },
    },
    // add the async callback to create a video client
    // for incoming calls in the background on a push notification
    createStreamVideoClient: async () => {
      const userId = await AsyncStorage.getItem("@userId");
      const userName = await AsyncStorage.getItem("@userName");
      if (!userId) return undefined;
      // an example promise to fetch token from your server
      const tokenProvider = async (): Promise<string> =>
        yourServerAPI.getTokenForUser(userId).then((auth) => auth.token);
      const user: User = { id: userId, name: userName };
      return StreamVideoClient.getOrCreateInstance({
        apiKey: STREAM_API_KEY, // pass your stream api key
        user,
        tokenProvider,
      });
    },
  });
}
It is essential that StreamVideoClient.getOrCreateInstance(..) is always used to get a existing StreamVideoClient instance in your app instead of always creating a new instance with new StreamVideoClient(..). Reusing the client instance makes sure that the accept/decline states of the call that are changed while app was in the background is preserved. The getOrCreateInstance method ensures that for the same user the instance is reused.

We highly recommend that android.smallIcon is set. Expo prebuild will automatically generate a notification icon with the correct settings from the app icon. It should be named as notification_icon.png. You can also set a different icon using the notification.icon property in your app.json or app.config.js. The custom icon should be 96x96 png grayscale with transparency.

Initialize SDK push notification methods
Call the methods we have created outside your application cycle. That is, alongside your AppRegistry.registerComponent() method call at the entry point of your application code.

We do this because the app can be opened from a dead state through a push notification, and in that case, we need to use the configuration and notification callbacks as soon as the JS bridge is initialized.

index.js
import "expo-router/entry";
import { setPushConfig } from "src/utils/setPushConfig";
import { setFirebaseListeners } from "src/utils/setFirebaseListeners";
setPushConfig(); // Set push config
setFirebaseListeners(); // Set the firebase listeners
Request for notification permissions in Android
At an appropriate place in your app, request for notification permissions from the user on Android. Below is a small example of how to request permissions in Expo:


import { PermissionsAndroid } from "react-native";
PermissionsAndroid.request(PermissionsAndroid.PERMISSIONS.POST_NOTIFICATIONS);
Disabling push notifications
In some cases, you would want to disable the delivery of push notifications. One example is the user logs out of your app or, if you switch an user on the device.


import { StreamVideoRN } from "@stream-io/video-react-native-sdk";
await StreamVideoRN.onPushLogout();
Optional: Android full-screen incoming call view on locked phone
Passing true to ringingPushNotifications.showWhenLockedAndroid will add the USE_FULL_SCREEN_INTENT permission to the android app and add the necessary configurations to the MainActivity.

For apps installed on phones running versions Android 13 or lower, the USE_FULL_SCREEN_INTENT permission is enabled by default.

For all apps being installed on Android 14 and above, the Google Play Store revokes the USE_FULL_SCREEN_INTENT for apps that do not have calling or alarm functionalities. Which means, while submitting your app to the play store, if you do declare that ‘Making and receiving calls’ is a ‘core’ functionality in your app, this permission is granted by default on Android 14 and above.

If the USE_FULL_SCREEN_INTENT permission is not granted, the notification will show up as an expanded heads up notification on the lock screen.

Incoming and Outgoing call UI in foreground
The last part of the setup for ringing calls is to show the incoming and outgoing call UIs in the app whenever there is a ringing call. If this was not implemented before, please visit this page of our documentation to implement that.

Troubleshooting
Check our Troubleshooting guide for solutions to common mistakes.

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
React Native
Next
React Native
© Getstream.io, Inc. All Rights Reserved.
Video and Audio
/
Docs
/
React Native
/
Expo
Expo
If you are on version 1.2 or below, you would need to upgrade to 1.3 or above to follow the setup. As 1.3 release had breaking changes with respect to setting up of push notifications. We recommend to update to the current latest version.

This guide discusses how to set up your Expo app to get push notifications from Stream for the non-ringing calls that your user will receive.

Add push provider credentials to Stream
Please follow the below guides for adding appropriate push providers to Stream:

Android - Firebase Cloud Messaging
iOS - Apple Push Notification Service (APNs)
Install Dependencies
Terminal
npx expo install expo-notifications
npx expo install expo-task-manager
npx expo install @notifee/react-native
So what did we install precisely?

expo-notifications and expo-task-manager for handling incoming Firebase Cloud Messaging notifications on Android and iOS.
@notifee/react-native - is used to customize and display push notifications.
Add Firebase credentials
To create a Firebase project, go to the Firebase console and click on Add project.

In the console, click the setting icon next to Project overview and open Project settings. Then, under Your apps, click the Android icon to open Add Firebase to your Android app and follow the steps. Make sure that the Android package name you enter is the same as the value of android.package from your app.json.

After registering the app, download the google-services.json file and place it in your project’s root directory.

In app.json, add an android.googleServicesFile field with the relative path to the downloaded google-services.json file. If you placed it in the root directory, the path is:

app.json
{
  "android": {
    "googleServicesFile": "./google-services.json"
  }
}
The google-services.json file contains unique and non-secret identifiers of your Firebase project. For more information, see Understand Firebase Projects.

iOS - Notifications entitlement
On Expo SDK 51 above, Notifications entitlement is no longer always added to iOS projects. Therefore, the following entitlements property needs to be added to app.json:

app.json
{
  "expo": {
    "ios": {
      "entitlements": {
        "aps-environment": “production”
      }
    }
  }
}
Add the config plugin property
In app.json, in the plugins field, add true to the enableNonRingingPushNotifications property in the @stream-io/video-react-native-sdk plugin.

app.json
{
 "plugins": [
      [
        "@stream-io/video-react-native-sdk",
        {
           "enableNonRingingPushNotifications": true
        }
      ],
      // your other plugins
  ]
}
If Expo EAS build is not used, please do npx expo prebuild --clean to generate the native directories again after adding the config plugins.

Setup the push config for the SDK
The SDK automatically processes the non ringing call push notifications once the setup above is done if the push config has been set using StreamVideoRN.setPushConfig. To do this follow the steps below,

Add the ability to statically navigate to screens in your app
When a user taps on the push notification and the JS engine is not ready, they should still be able to navigate to the screen that shows the active call. You can achieve this by using imperative navigation in the expo router.

The following is an example implementation of a utility file that has helpers to statically navigate in the app:

src/utils/staticNavigation.ts
import { User } from "@stream-io/video-react-native-sdk";
import { router } from "expo-router";
/**
 * This is used to run the navigation logic from root level
 */
export const staticNavigateToActiveCall = () => {
  const intervalId = setInterval(async () => {
    // add any requirements here (like authentication)
    if (GlobalState.hasAuthentication) {
      clearInterval(intervalId);
      router.push("/activecall");
    }
  }, 300);
};
export const staticNavigateToLivestreamCall = () => {
  const intervalId = setInterval(async () => {
    // add any requirements here (like authentication)
    if (GlobalState.hasAuthentication) {
      clearInterval(intervalId);
      router.push("/livestream");
    }
  }, 300);
};
Add Push message handlers
To process the incoming push notifications, the SDK provides the utility functions that you must add to your existing or new notification listeners.

Add callbacks to process notifications and displaying it
To process notifications on Android, we can use either @react-native-firebase library or expo-task-manager. The disadvantage of expo-task-manager is that it does not work when push is delivered to an app that has its underlying process in a killed state. So we recommend using the @react-native-firebase library. For iOS, we only need the expo-notifications library.


@react-native-firebase

expo-task-manager
First we have to install the react-native-firebase library.

Terminal
yarn add @react-native-firebase/app
yarn add @react-native-firebase/messaging
Below is the snippet to add message handlers:

src/utils/setPushMessageHandlers.ts
import messaging from "@react-native-firebase/messaging";
import {
  isFirebaseStreamVideoMessage,
  firebaseDataHandler,
} from "@stream-io/video-react-native-sdk";
import { Platform } from "react-native";
import * as Notifications from "expo-notifications";
export const setPushMessageListeners = () => {
  // Set up the background message handler for Android
  messaging().setBackgroundMessageHandler(async (msg) => {
    if (isFirebaseStreamVideoMessage(msg)) {
      await firebaseDataHandler(msg.data);
    } else {
      // your other messages (if any)
    }
  });
  // Set up the foreground message handler for Android
  messaging().onMessage((msg) => {
    if (isFirebaseStreamVideoMessage(msg)) {
      firebaseDataHandler(msg.data);
    } else {
      // your other messages (if any)
    }
  });
  if (Platform.OS === "ios") {
    // show notification on foreground on iOS
    Notifications.setNotificationHandler({
      // example configuration below to show alert and play sound
      handleNotification: async (notification) => ({
        shouldShowAlert: true,
        shouldPlaySound: true,
        shouldSetBadge: false,
      }),
    });
  }
};
The Firebase message handlers

The isFirebaseStreamVideoMessage method is used to check if this push message is a video related message. And only this needs to be processed by the SDK.
The firebaseDataHandler method is the callback to be invoked to process the message. This callback reads the message and uses the @notifee/react-native library to display push notifications.
Disable Firebase initialisation on iOS

React Native Firebase Messaging automatically registers the device with APNs to receive remote messages. But since we do not use Firebase on iOS, we can disable it via the firebase.json file that we can newly create:

{projectRoot}/firebase.json
{
  "react-native": {
    "messaging_ios_auto_register_for_remote_messages": false
  }
}
Add notification button listeners
Below is the snippet of how to add the notification button listeners:

src/utils/setNotifeeListeners.ts
import {
  isNotifeeStreamVideoEvent,
  onAndroidNotifeeEvent,
  oniOSNotifeeEvent,
} from "@stream-io/video-react-native-sdk";
import { Platform } from "react-native";
import notifee from "@notifee/react-native";
export const setNotifeeListeners = () => {
  // on press handlers of background notifications for Android
  notifee.onBackgroundEvent(async (event) => {
    if (isNotifeeStreamVideoEvent(event)) {
      await onAndroidNotifeeEvent({ event, isBackground: true });
    } else {
      // your other notifications (if any)
    }
  });
  // on press handlers of foreground notifications for Android
  notifee.onForegroundEvent((event) => {
    if (Platform.OS === "android" && isNotifeeStreamVideoEvent(event)) {
      onAndroidNotifeeEvent({ event, isBackground: false });
    } else {
      // your other notifications (if any)
    }
  });
};
The Notifee event handlers

The isNotifeeStreamVideoEvent method is used to check if the event was a video related notifee event. And only this needs to be processed by the SDK.
The onAndroidNotifeeEvent method is the callback to be invoked to process the event. This callback reads the event and makes sure that the call is accepted or declined.
Adding handler for iOS

Add the following useEffect in the root component of your App, this is most likely in App.tsx.

App.tsx
import * as Notifications from "expo-notifications";
useEffect(() => {
  if (Platform.OS === "ios") {
    const subscription = Notifications.addNotificationReceivedListener(
      (notification) => {
        if (isExpoNotificationStreamVideoEvent(notification)) {
          oniOSExpoNotificationEvent(notification);
        } else {
          // your other notifications (if any)
        }
      },
    );
    return () => {
      subscription.remove();
    };
  }
}, []);
Setup the push config
Once we have set up the methods to navigate the app from a static method, we are ready to call the StreamVideoRN.setPushConfig method. Through this method, you can override the default notification texts and set the push provider name for both iOS and Android and provide your custom ringtone.

Below is an example of how this method can be called:

src/utils/setPushConfig.ts
import {
  StreamVideoClient,
  StreamVideoRN,
} from "@stream-io/video-react-native-sdk";
import { AndroidImportance } from "@notifee/react-native";
import AsyncStorage from "@react-native-async-storage/async-storage";
import { STREAM_API_KEY } from "../../constants";
import {
  staticNavigateToRingingCall,
  staticNavigateToLivestreamCall,
} from "./staticNavigationUtils";
export function setPushConfig() {
  StreamVideoRN.setPushConfig({
    // pass true to inform the SDK that this is an expo app
    isExpo: true,
    ios: {
      // add your push_provider_name for iOS that you have setup in Stream dashboard
      pushProviderName: __DEV__ ? "apn-video-staging" : "apn-video-production",
    },
    android: {
      // the name of android notification icon (Optional, defaults to 'ic_launcher')
      smallIcon: "ic_notification",
      // add your push_provider_name for Android that you have setup in Stream dashboard
      pushProviderName: __DEV__
        ? "firebase-video-staging"
        : "firebase-video-production",
      // configure the notification channel to be used for non ringing calls for Android.
      callChannel: {
        id: "stream_call_notifications",
        name: "Call notifications",
        // This importance will ensure that the notification will appear on-top-of applications.
        importance: AndroidImportance.HIGH,
        sound: "default",
      },
      // configure the functions to create the texts shown in the notification
      // for non ringing calls in Android.
      callNotificationTextGetters: {
        getTitle(type, createdUserName) {
          if (type === "call.live_started") {
            return `Call went live, it was started by ${createdUserName}`;
          } else {
            return `${createdUserName} is notifying you about a call`;
          }
        },
        getBody(_type, createdUserName) {
          return "Tap to open the call";
        },
      },
    },
    // optional: add the callback to be executed when a non ringing call notification is tapped
    onTapNonRingingCallNotification: () => {
      const [callType, callId] = call_cid.split(":");
      if (callType === "livestream") {
        staticNavigateToLivestreamCall();
      } else {
        staticNavigateToActiveCall();
      }
    },
    // add the async callback to create a video client
    // for incoming calls in the background on a push notification
    createStreamVideoClient: async () => {
      // note that since the method is async,
      // you can call your server to get the user data or token or retrieve from offline storage.
      const userId = await AsyncStorage.getItem("@userId");
      const userName = await AsyncStorage.getItem("@userName");
      // an example promise to fetch token from your server
      const tokenProvider = () =>
        yourServer.getTokenForUser(userId).then((auth) => auth.token);
      const user = { id: userId, name: userName };
      return StreamVideoClient.getOrCreateInstance({
        apiKey: STREAM_API_KEY, // pass your stream api key
        user,
        tokenProvider,
      });
    },
  });
}
It is essential that StreamVideoClient.getOrCreateInstance(..) is always used to get a existing StreamVideoClient instance in your app instead of always creating a new instance with new StreamVideoClient(..). Reusing the client instance makes sure that the accept/decline states of the call that are changed while app was in the background is preserved. The getOrCreateInstance method ensures that for the same user the instance is reused.

We highly recommend that android.smallIcon is set. Expo prebuild will automatically generate a notification icon with the correct settings from the app icon. It should be named as notification_icon.png. You can also set a different icon using the notification.icon property in your app.json or app.config.js. The custom icon should be 96x96 png grayscale with transparency.

Call the created methods outside of the application lifecycle
Call the methods we have created outside of your application cycle. That is inside index.js or the equivalent entry point file. This is because the app can be opened from a dead state through a push notification and in that case, we need to use the configuration and notification callbacks as soon as the JS bridge is initialized.

Following is an example,

index.js
import "expo-router/entry";
import { setPushConfig } from "src/utils/setPushConfig";
import { setNotifeeListeners } from "src/utils/setNotifeeListeners";
import { setPushMessageListeners } from "src/utils/setPushMessageListeners";
setPushConfig();
setNotifeeListeners();
setPushMessageListeners();
Request for notification permissions
At an appropriate place in your app, request for notification permissions from the user. Below is a small example of how to request permissions in Expo:


import * as Notifications from "expo-notifications";
await Notifications.requestPermissionsAsync();
Disabling push - usually on logout
In some cases you would want to disable push from happening. For example, if user logs out of your app. Or if the user switches. You can disable push like below:


import { StreamVideoRN } from "@stream-io/video-react-native-sdk";
await StreamVideoRN.onPushLogout();
Troubleshooting
If you encounter any issues, refer to the Troubleshooting guide for solutions to common mistakes.

Did you find this page helpful?
It was helpful
It was not helpful
I have feedback
Previous
React Native
Next
Troubleshooting
© Getstream.io, Inc. All Rights Reserved.
